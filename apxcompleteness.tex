\documentclass[]{article}

% Package `hyperref` must come before package `complexity`.
\usepackage[pdftitle={Revisiting the hardness of approximation of Maximum 3-Satisfiability}, pdfauthor={Jeffrey Finkelstein}]{hyperref}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{complexity}

\theoremstyle{plain}
\newtheorem{corollary}{Corollary}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{todo}{TODO}

\theoremstyle{definition}
\newtheorem{definition}{Definition}

\newenvironment{instance}{\\Instance:}{}
\newenvironment{measure}{\\Measure:}{}
\newenvironment{proofidea}{\begin{proof}[Proof idea]}{\end{proof}}
\newenvironment{solution}{\\Solution:}{}
\newenvironment{question}{\\Question:}{}

\newcommand{\algorithmautorefname}{Algorithm}
\newcommand{\corollaryautorefname}{Corollary}
\newcommand{\definitionautorefname}{Definition}
\newcommand{\lemmaautorefname}{Lemma}

\algrenewcommand\algorithmicrequire{\textbf{Input:}}

\newcommand{\AND}{\textsf{AND}}
\newcommand{\OR}{\textsf{OR}}
\newcommand{\NOT}{\textsf{NOT}}

\newcommand{\APr}{\leq_{AP}^{P}}
\newcommand{\NCAPr}{\leq_{AP}^{NC}}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}}}
\newcommand{\mornt}{\leq_m}
\newcommand{\mor}{\mornt^P}
\newcommand{\mornc}{\mornt^{NC}}
\newcommand{\pair}[2]{{\left\langle{#1}, {#2}\right\rangle}}

\author{Jef{}frey Finkelstein}
\date{\today}
\title{Revisiting the hardness of approximation of \texorpdfstring{\textsc{Maximum 3-Satisfiability}}{Maximum 3-Satisfiability}}

\begin{document}

\maketitle

\section{Introduction}

In this work, we attempt to clarify the existing proofs that \textsc{Maximum 3-Satisfiability} is both hard to approximate \emph{in polynomial time} and complete for the class \APX{} under an appropriate type of reduction.
By doing this, we hope to provide not only a better understanding of the original proofs, but also a platform on which to base similar proofs.
We use this platform to provide a clarification of the existing proofs that \textsc{Maximum 3-Satisfiability} is both hard to approximate \emph{by efficient and highly parallel algorithms} and complete for the class \NCX{} under an appropriate type of reduction.

We introduce the necessary definitions and basic facts in \autoref{sec:prelim}.
In \autoref{ssc:phardness}, we present a proof that \textsc{Maximum 3-Satisfiability} is hard to approximate in polynomial time within a constant factor, for sufficiently small constants.
The proof here is a careful rewrite of the proofs given in \cite[Theorem~6.3]{book} and \cite[Corollary~29.8]{vazirani}.
The corresponding proof for highly parallel algorithms is in \autoref{ssc:nchardness}.
In \autoref{ssc:apxcomplete} we present a proof that \textsc{Maximum 3-Satisfiability} is complete for the class of polynomial time constant factor approximable optimization problems, under an appropriate type of reduction.
The proof here is a careful rewrite of the proof given in \cite[Theorem~8.6]{book}.
The corresponding proof for highly parallel algorithms is in \autoref{ssc:ncxcomplete}.

\section{Preliminaries}\label{sec:prelim}

Throughout this work, $\Sigma=\{0, 1\}$, and $\Sigma^*$ is the set of all finite binary strings.
The length of a string $x \in \Sigma^*$ is denoted $|x|$.
We denote the set of positive rational numbers by $\mathbb{Q}^*$ and the natural numbers (excluding 0) by $\mathbb{N}$.
A decision problem is a subset of $\Sigma^*$.
A binary relation $R \subseteq \Sigma^* \times \Sigma^*$ is \emph{polynomially bounded} if there exists a polynomial $p$ such that for all $(x, y) \in R$, we have $|y| \leq p(|x|)$.

\subsection{Complexity classes}

\NP{} is the class of decision problems $L$ for which there exists a polynomially bounded \emph{solution relation} (or \emph{\NP{} relation}) $R$ such that
\begin{enumerate}
\item $R$ is decidable by a deterministic Turing machine running in polynomial time, and
\item for all $x \in \Sigma^*$, we have $x \in L$ if and only if there exists a $y$ such that $(x, y) \in R$.
\end{enumerate}
\P{} is the subclass of \NP{} in which each decision problem $L$ is decidable by a deterministic Turing machine running in polynomial time.
\NC{} is the subclass of \P{} in which each decision problem is decidable by a logarithmic space uniform family of Boolean circuits (consisting of \AND, \OR, and \NOT gates, each with fan-in at most two) of polynomial size and polylogarithmic depth.
\FNC{} is the class of functions computable by an \NC{} circuit family.
\NNC{} is the class of decision problems $L$ for which there exists a polynomially bounded \emph{solution relation} (or \emph{\NNC{} relation}) $R$ such that
\begin{enumerate}
\item $R$ is decidable by an \NC{} circuit family, and
\item for all $x \in \Sigma^*$, we have $x \in L$ if and only if there exists a $y$ such that $(x, y) \in R$.
\end{enumerate}
(A different but equivalent definition for \NNC{} is given in \cite{wolf94}, where it is called \NNCpoly.)

The class of functions computable in polynomial time is denoted \FP.
The class of functions computable by a \NC{} circuit family is denoted \FNC.

\begin{theorem}[{\cite{wolf94}}]\label{thm:nncequalsnp}
  $\NNC = \NP$.
\end{theorem}

\begin{lemma}
  \mbox{}
  \begin{enumerate}
  \item If $f \in \FP$ and $g \in \FP$, then $f \circ g \in \FP$.
  \item If $f \in \FNC$ and $g \in \FNC$, then $f \circ g \in \FNC$.
  \end{enumerate}
\end{lemma}

\begin{lemma}
  The addition, subtraction, multiplication, division, and exponentiation functions are all computable by \NC{} circuit families.
\end{lemma}

\subsection{Reductions among decision problems}

If $P$ and $Q$ are two decision problems, we say \emph{$P$ many-one reduces to $Q$} and write $P \mornt Q$ if there exists a function $f$ such that $x \in P$ if and only if $f(x) \in Q$.
If $f$ is computable in polynomial time (respectively, \NC) then we say the reduction is a polynomial time (respectively, \NC) many-one reduction and write $P \mor Q$ (respectively, $P \mornc Q$).
For any complexity class $\mathcal{C}$ and $Q \in \mathcal{C}$, if for all problems $P \in \mathcal{C}$ we have $P$ many-one reduces to $Q$, then we say $Q$ is complete for $\mathcal{C}$ under many-one reductions.

\subsection{Probabilistically checkable proofs}

A \emph{$(r(n), q(n))$-verifier} is a probabilistic polynomial time Turing machine that, on input $(x, \pi, \rho)$ where $|x| = n$, $\pi$ is interpreted as a \emph{proof} (also known as a \emph{witness} or \emph{certificate}) and $\rho$ is interpreted as a sequence of random bits, reads at most $O(r(n))$ random bits from $\rho$ and reads (with random access) at most $O(q(n))$ bits from $\pi$.
Define the complexity class $\PCP(r(n), q(n))$ (for \emph{probabilistically checkable proofs} with $O(r(n))$ random bits and $O(q(n))$ proof queries) as follows: a decision problem $L \in \PCP(r(n), q(n))$ if there exists a $(r(n), q(n))$-verifier $V$ and a polynomial $p$ such that
\begin{enumerate}
\item
  if $x \in L$ then there exists a $\pi \in \Sigma^*$ with $|\pi| \leq p(|x|)$ such that
  \begin{displaymath}
    \Pr_{\rho \in \Sigma^{p(|x|)}}\left[V(x, \pi; \rho) \textnormal{ accepts}\right] = 1,
  \end{displaymath}
  and
\item
  if $x \notin L$ then for all $\pi \in \Sigma^*$ with $|\pi| \leq p(|x|)$, we have
  \begin{displaymath}
    \Pr_{\rho \in \Sigma^{p(|x|)}}\left[V(x, \pi; \rho) \textnormal{ accepts}\right] < \frac{1}{2}.
  \end{displaymath}
\end{enumerate}

\begin{theorem}[PCP Theorem \cite{pcp}]\label{thm:pcp}
  $\NP = \PCP(\lg n, 1)$.
\end{theorem}

\subsection{Optimization problems}

An optimization problem is given by $(I, S, m, t)$, where $I$ is called the \emph{instance set}, $S \subseteq I \times \Sigma^*$ and is called the \emph{solution relation}, $m \colon I \times \Sigma^* \to \mathbb{N}$ and is called the \emph{measure function}, and $t\in\{\max, \min\}$ and is called the \emph{type} of the optimization.
The set of solutions corresponding to an instance $x \in I$ is denoted $S_P(x)$.
The \emph{optimal measure} for any $x \in I$ is denoted $m^*(x)$.
Observe that for all $x \in I$ and all $y \in \Sigma^*$, if $t = \max$ then $m(x, y) \leq m^*(x)$ and if $t = \min$ then $m(x, y) \geq m^*(x)$.
The \emph{performance ratio}, $R$, of a solution $y$ for $x$ is
\begin{displaymath}
  R(x, y) = \max{\left(\frac{m^*(x)}{m(x, y)}, \frac{m(x, y)}{m^*(x, y)}\right)}.
\end{displaymath}
A function $f \colon I \to \Sigma^*$ is an \emph{$r(n)$-approximator} if $R(x, f(x)) \leq r(|x|)$, for some function $r \colon \mathbb{N} \to \mathbb{Q}^*$.
If $r(n)$ is a constant function with value $\delta$, we simply say $f$ is a \emph{$\delta$-approximator}.

For the sake of simplicity, in \autoref{ssc:gaps} and \autoref{sec:hardness} (the sections dealing with hardness of approximation) we will consider \emph{only} maximization problems, although the definitions and results also hold for minimization problems, with the appropriate (slight) changes to the definitions (for example, in \autoref{def:intro} we would require that $m^*(f(x)) > (1 + \epsilon) \cdot c(x)$ instead of $m^*(f(x)) < \frac{1}{1 + \epsilon} \cdot c(x)$).

\subsubsection{Classes of optimization problems}

The complexity class \NPO{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
\begin{enumerate}
\item The instance set $I$ is decidable by a deterministic polynomial time Turing machine.
\item The solution relation $S$ is decidable by a deterministic polynomial time Turing machine and is polynomially bounded.
\item The measure function $m$ is computable by a deterministic polynomial time Turing machine.
\end{enumerate}
The complexity class \APX{} is the subclass of \NPO{} in which for each optimization problem $P$ there exists a polynomial time computable $r$-approximator for $P$, where $r(n) \in O(1)$ for all $n \in \mathbb{N}$.
The complexity class \PTAS{} is the subclass of \APX{} in which for each optimization problem $P$, defined by $P = (I, S, m, t)$, there exists a function $f \colon I \times \mathbb{N} \to \Sigma^*$ such that $R(x, f(x, k)) \leq 1 + \frac{1}{k}$ for all $x \in I$ and all $k \in \mathbb{N}$, and $f$ is computable in polynomial time with respect to the length of $x$.

The complexity class \NNCO{} is the class of optimization problems $(I, S, m, t)$ such that the following conditions hold.
\begin{enumerate}
\item The instance set $I$ is decidable by an \NC{} circuit family.
\item The solution relation $S$ is decidable by an \NC{} circuit family and is polynomially bounded.
\item The measure function $m$ is computable by an \FNC{} circuit family.
\end{enumerate}
The complexity class \NCX{} is the subclass of \NNCO{} in which for each optimization problem $P$ there exists an $r$-approximator, computable by an \FNC{} circuit family, for $P$, where $r(n) \in O(1)$ for all $n \in \mathbb{N}$.
The complexity class \NCAS{} is the subclass of \NCX{} in which for each optimization problem $P$, defined by $P = (I, S, m, t)$, there exists a function $f \colon I \times \mathbb{N} \to \Sigma^*$ such that $R(x, f(x, k)) \leq 1 + \frac{1}{k}$ for all $x \in I$ and all $k \in \mathbb{N}$, and $f$ is computable by an \FNC{} circuit family with respect to the length of $x$.

Note that since $\NNC \subseteq \NP$, it follow that $\NNCO \subseteq \NPO$.

\subsubsection{Reductions among optimization problems}

\begin{definition}[{\cite[Definition~8.3]{book}}]
  Let $P$ and $Q$ be optimization problems, so $P = (I_P, S_P, m_P, t_P)$ and $Q = (I_Q, S_Q, m_Q, t_Q)$.
  There is an \emph{AP reduction from $P$ to $Q$}, denoted $P \APr Q$, if there exist functions $f \colon I_P \times \mathbb{Q}^+ \to I_Q$ and $g \colon I_P \times \Sigma^* \times \mathbb{Q}^+ \to \Sigma^*$ and constant $\alpha \in \mathbb{Q}^+$ with $\alpha \geq 1$ such that:
  \begin{enumerate}
  \item For all $x \in I_P$ and all $r \in \mathbb{Q}^+$ with $r > 1$, we have $f(x, r) \in I_P$.
  \item For all $x \in I_P$ and all $r \in \mathbb{Q}^+$ with $r > 1$, if $S_P(x) \neq \emptyset$ then $S_Q(f(x, r)) \neq \emptyset$.
  \item For all $x \in I_P$, all $r \in \mathbb{Q}^+$ with $r > 1$, and all $y \in S_Q(f(x, r))$, we have $g(x, y, r) \in S_P(x)$.
  \item For all $x \in I_P$, all $r \in \mathbb{Q}^+$ with $r > 1$, and all $y \in S_Q(f(x, r))$, we have
    \begin{equation*}
      R_Q(f(x, r), y) \leq r \text{ implies } R_P(x, g(x, y, r)) \leq 1 + \alpha \cdot (r - 1).
    \end{equation*}
  \end{enumerate}
  We identify such a reduction with the triple $(f, g, \alpha)$.

  If, furthermore, $f$ is computable in polynomial time with respect to $x$ and $g$ is computable in polynomial time with respect to $x$ and $y$, then the reduction is called a \emph{polynomial time AP reduction}.
  If $f$ and $g$ are computable by \FNC{} circuit families, then the reduction is called an \emph{\NC{} AP reduction}.
\end{definition}

Although the definition of AP reduction allows $f$ and $g$ access to $r$, the quality of the approximation, in most known natural reductions, this access is not necessary.
Therefore, in some cases, we will write $f(x)$ and $g(x, y)$ instead of $f(x, r)$ and $g(x, y, r)$.

There are a wealth of other kinds of reductions among optimization problems designed to capture the notion that a good approximate solution for problem $Q$ implies a good approximate solution for problem $P$.
Some experts suggest that the AP reduction is the correct reduction to use to show, for example, completeness in \APX.
For a survey of other approximation preserving reductions, see \cite{crescenzi97}.

\subsection{Boolean formulae}

A \emph{literal} is a variable or its negation.
A Boolean formula is in \emph{conjunctive normal form (CNF)} if it is a conjunction of clauses, each of which is a disjunction of literals.
A Boolean formula is called \emph{$k$-CNF} if it is in conjunctive normal form and each of the clauses has at most $k$ literals.
The set of variables which appear in a Boolean formula is $Var(\phi)$.
A \emph{truth assignment} to a Boolean formula $\phi$ is a function $\tau \colon Var(\phi) \to \{0, 1\}$.
A truth assignment \emph{satisfies} a Boolean formula $\phi$ if replacing the variables in $\phi$ with their corresponding truth assignments yields a true formula.

\begin{definition}[\textsc{Satisfiability}]
  \mbox{}
  \begin{instance}
    a Boolean formula $\phi$
  \end{instance}
  \begin{question}
    Is there a truth assignment $\tau$ to $\phi$ such that $\tau$ satisfies $\phi$?
  \end{question}
\end{definition}

\begin{definition}[\textsc{3-Satisfiability}]
  \mbox{}
  \begin{instance}
    a 3-CNF Boolean formula $\phi$
  \end{instance}
  \begin{question}
    Is there a truth assignment $\tau$ to $\phi$ such that $\tau$ satisfies $\phi$?
  \end{question}
\end{definition}

\begin{definition}[\textsc{Maximum 3-Satisfiability}]
  \mbox{}
  \begin{instance}
    a 3-CNF Boolean formula $\phi$
  \end{instance}
  \begin{solution}
    a truth assignment $\tau$ to $\phi$
  \end{solution}
  \begin{measure}
    the number of satisfied clauses in $\phi$
  \end{measure}
\end{definition}

\begin{lemma}[Cook-Levin Theorem]\label{lem:cooklevin}
  There exists a logarithmic-space (and more generally an \NC{} and \P) computable function $C$ which takes two inputs, the description of a Turing machine $B$ and a tuple of its arguments $(x_1, x_2, \dotsc, x_k)$, and outputs a Boolean formula $\phi$ whose variables represent the bits of the arguments to $C$, such that $B(x_1, x_2, \dotsc, x_k)$ accepts if and only if $\phi$ is satisfiable.
\end{lemma}

\begin{todo}
  State the type of Turing machine that $B$ must be (must it be a nondeterministic polynomial time Turing machine?).
  State the length of the description of $B$.
  State the length of the output $\Phi$.
\end{todo}

\begin{todo}
  Find a reference which explicitly states that the Cook-Levin reduction is computable in logarithmic space.
\end{todo}

\begin{lemma}\label{lem:satisnnccomplete}
  $\textsc{Satisfiability}$ is complete for \NNC{} under \NC{} many-one reductions, and more generally it is complete for \NP{} under polynomial time many-one reductions.
\end{lemma}
\begin{proof}
  Since $\NP = \NNC$ and $\NC \subseteq \P$ it suffices to show that the problem is complete in $\NP$ under $\NC$ reductions.
  By \autoref{lem:cooklevin}, there is a logarithmic space computable (and hence \NC{} computable) function which maps any nondeterministic polynomial time Turing machine to an equivalent Boolean formula.
  This function satisfies the requirements of an \NC{} many-one reduction.
\end{proof}

\begin{lemma}\label{lem:half}
 At least half of the clauses of any Boolean formula in conjunctive normal form are always satisfiable.
\end{lemma}

\begin{lemma}[{\cite[Example~6.5]{book}}]\label{lem:three}
  There is an \NC{} computable function $T$ that maps a Boolean formula in conjunctive normal form with $c$ clauses and at most $k$ literals per clause into an equivalent Boolean formula in conjunctive normal form with $c \cdot (k - 2)$ clauses and at most three literals per clause.
\end{lemma}

\begin{lemma}\label{lem:evaluation}
  There is a logarithmic space (more generally, a polynomial time) Turing machine which, when given Boolean formula $\phi$ and truth assignment $\tau$, whether $\tau$ satisfies $\phi$.
\end{lemma}
\begin{todo}
  Is this true?
  It should be by a simple recursive procedure:
  \begin{enumerate}
  \item if single bit, output that bit, otherwise
  \item recursively evaluate left subtree
  \item recursively evaluate right subtree
  \item combine left and right according to gate type
  \end{enumerate}
  Boolean formula evaluation is a special case of Boolean circuit evaluation, which is \P-complete, so I just want to be sure.
\end{todo}

\begin{lemma}\label{lem:maxsatnnco}
  $\textsc{Maximum 3-Satisfiability}$ is in \NNCO, and more generally in $\NPO$.
\end{lemma}
\begin{proof}
  It is clear that the instance set is decidable in \NC.
  By \autoref{lem:evaluation}, the solution set is decidable in \NC.
  To compute the measure function, evaluate each of the clauses and then compute the total number of those that are satisfied.
  Since Boolean function evaluation is in \NC{} and computing the sum of $m$ numbers is in \NC, the measure function is computable by an \NC{} circuit family.
\end{proof}

\subsection{Gap-introducing and gap-preserving reductions}\label{ssc:gaps}

\begin{definition}[{\cite[Section~29.1]{vazirani}}]\label{def:intro}
  Let $P$ be a decision problem and $Q$ be maximization problem with $Q = (I, S, m, \max)$.
  There is a \emph{gap-introducing reduction} from $P$ to $Q$ if there exist functions $f \colon \Sigma^* \to I$ and $c \colon \Sigma^* \to \mathbb{N}$ and an $\epsilon \in \mathbb{Q}^+$ such that for any $x \in \Sigma^*$,
  \begin{enumerate}
  \item if $x \in P$ then $m^*(f(x)) \geq c(x)$, and
  \item if $x \notin P$ then $m^*(f(x)) < \frac{1}{1 + \epsilon} \cdot c(x)$.
  \end{enumerate}
  We call $\epsilon$ the \emph{gap parameter} of the reduction.

  If, furthermore, $f$ and $c$ are computable in polynomial time, then the reduction is called a \emph{polynomial time gap-introducing reduction}.
  If $f$ and $c$ are computable by \FNC{} circuit families, then the reduction is called an \emph{\NC{} gap-introducing reduction}.
\end{definition}

Notice that as the value of $\epsilon$ increases, so does the ``gap'' between the optimal measures corresponding to strings in $P$ and strings not in $P$.

\begin{definition}
  Let $P\in\NP$ and $Q$ be a maximization problem in \NPO, with $Q = (I, S, m, \max)$.
  Let $R_P$ be the \NP{} relation induced by $P$.
  Suppose there is a polynomial time (respectively, \NC) gap-introducing reduction $(f, c, \epsilon)$ from $P$ to $Q$.
  The gap-introducing reduction is \emph{enhanced} if there exists a polynomial time (respectively, \NC) computable function $g \colon I \times \Sigma^* \to \Sigma^*$ such that for all $x \in I$ and all $y \in S(f(x))$ with $m(f(x), y) \geq \frac{1}{1 + \epsilon} \cdot c(x)$, we have $\left(x, g(x, y)\right) \in R_P$ if and only if $x \in P$.
\end{definition}

An enhanced gap-introducing reduction is a gap-introducing reduction by definition.
Notice that from \autoref{lem:xinp} it is vacuously true that $\left(x, g(x, y)\right) \in R_P$ implies $x \in P$, since the conclusion is always true.
The interesting case is the converse, because it requires that $g$ produce a witness that $x$ is in $P$ even though it may only have an approximate solution $y$.

\begin{lemma}\label{lem:xinp}
  If there is a gap-introducing reduction $(f, c, \epsilon)$ from $P$ to $Q$, where $P = (I_P, S_P, m_P, \max)$ and $Q = (I_Q, S_Q, m_Q, \max)$, then for all $x \in I_P$ and all $y \in S_Q(f(x))$, if $m(f(x), y) \geq \frac{1}{1 + \epsilon} \cdot c(x)$ then $x \in P$.
\end{lemma}
\begin{proof}
  Since $m^*(f(x)) \geq m(f(x), y) \geq \frac{1}{1 + \epsilon} \cdot c(x)$, we have $x \in P$ by condition 2 of the gap-introducing reduction.
\end{proof}

\begin{definition}[{\cite[Section~29.1]{vazirani}}]
  Let $P$ and $Q$ be maximization problems, where $P = (I_P, S_P, m_P, \max)$ and $Q = (I_Q, S_Q, m_Q, \max)$.
  There is a \emph{gap-preserving reduction} from $P$ to $Q$ if there exist a function $f \colon I_P \to I_Q$, functions $c_P, c_Q \colon \mathbb{N} \to \mathbb{N}$, and constants $\alpha, \beta \in \mathbb{Q}^+$, with $\alpha \geq 0$ and $\beta \geq 0$, such that
  \begin{itemize}
  \item if $m^*_P(x) \geq c_P(x)$ then $m^*_Q(f(x)) \geq c_Q(f(x))$, and
  \item if $m^*_P(x) < \frac{1}{1 + \alpha} \cdot c_P(x)$ then $m^*_Q(f(x)) < \frac{1}{1 + \beta} \cdot c_Q(f(x))$.
  \end{itemize}
  We identify such a reduction with the five-tuple $(f, c_P, c_Q, \alpha, \beta)$.

  If, furthermore, $f$, $c_P$, and $c_Q$ are computable in polynomial time, then the reduction is called a \emph{polynomial time gap-introducing reduction}.
  If $f$, $c_P$, and $c_Q$ are computable by \FNC{} circuit families, then the reduction is called an \emph{\NC{} gap-introducing reduction}.
\end{definition}

Intuitively, the gap in $P$ scales with $\alpha$ and the gap in $Q$ with $\beta$.
Also, we note here that although Vazirani allows $\alpha$ and $\beta$ to be functions of the length of the input $x$, we require them to be constants both for consistency with the definition of the gap-introducing reduction and for simplicity since we will not need this relaxation.

Like the enhanced gap-introducing reduction, there is also an enhanced gap-preserving reduction.

\begin{definition}
  Let $P$ and $Q$ be two maximization problems, defined by $P = (I_P, S_P, m_P, \max)$ and $Q = (I_Q, S_Q, m_Q, \max)$.
  Suppose that there exists a polynomial time (respectively, \NC) gap-preserving reduction $(f, c_P, c_Q, \alpha, \beta)$ from $P$ to $Q$.
  The gap-preserving reduction is \emph{enhanced} if there exists a polynomial time (respectively, \NC) computable function $g \colon I_P \times \Sigma^* \to \Sigma^*$ such that for all $x \in I_P$ and all $y \in S_Q(f(x))$ with $m_Q(f(x), y) \geq \frac{1}{1 + \beta} \cdot c_Q(f(x))$, we have $g(x, y) \in S_P(x)$ and $m_P(x, g(x, y)) \geq \frac{1}{1 + \alpha} \cdot c_P(x)$.
  We identify such a reduction with the six-tuple $(f, g, c_P, c_Q, \alpha, \beta)$.
\end{definition}

\begin{lemma}\label{lem:compose}
  Let $P$ be a decision problem and $Q$ and $T$ be maximization problems.
  Suppose there is a polynomial time (respectively, \NC) enhanced gap-introducing reduction $(f, g, c, \epsilon)$ from $P$ to $Q$ and a polynomial time (respectively, \NC) enhanced gap-preserving reduction $(f', g', c_Q, c_T, \alpha, \beta)$ from $Q$ to $T$.
  If $c(x) \geq c_Q(f(x))$ and $\frac{1}{1 + \epsilon} \cdot c(x) \leq \frac{1}{1 + \alpha} \cdot c_Q(f(x))$ then $(\tilde{f}, \tilde{g}, \tilde{c}, \tilde{\epsilon})$ is a polynomial time (respectively, \NC) enhanced gap-introducing reduction from $P$ to $Q$, where
  \begin{align*}
    \tilde{f}(x) & = f'(f(x)), \\
    \tilde{g}(x, y) & = g(x, g'(f(x), y)), \\
    \tilde{c}(x) & = c_T(f'(f(x))), \text{ and} \\
    \tilde{\epsilon} & = \beta.
  \end{align*}
\end{lemma}
\begin{proof}
  Suppose first that $x \in P$.
  Our goal is to show that $m^*_T(f'(f(x))) \geq \tilde{c}(x)$.
  Since we have a gap-introducing reduction from $P$ to $Q$, we know $m^*_Q(f(x)) \geq c(x)$.
  Since we have a gap-preserving reduction from $Q$ to $T$, we know that if $m^*_Q(f(x)) \geq c_Q(f(x))$ then $m^*_T(f'(f(x))) \geq c_T(f'(f(x)))$.
  Since $c(x) \geq c_Q(f(x))$ by hypothesis, it follows that $m^*_T(f'(f(x))) \geq c_T(f'(f(x))) = \tilde{c}(x)$.

  Suppose now that $x \notin P$.
  Our goal is to show that $m^*_T(f'(f(x))) < \frac{1}{1 + \tilde{\epsilon}} \cdot \tilde{c}(x)$.
  Since we have a gap-introducing reduction from $P$ to $Q$, we know $m^*_Q(f(x)) < \frac{1}{1 + \epsilon} \cdot c(x)$.
  Since we have a gap-preserving reduction from $Q$ to $T$, we know that if $m^*_Q(f(x)) < \frac{1}{1 + \alpha} \cdot c_Q(f(x))$ then $m^*_T(f'(f(x))) < \frac{1}{1 + \beta} \cdot c_T(f'(f(x)))$.
  Since $\frac{1}{1 + \epsilon} \cdot c(x) \leq \frac{1}{1 + \alpha} \cdot c_Q(f(x))$ by hypothesis, it follows that $m^*_T(f'(f(x))) < \frac{1}{1 + \beta} \cdot c_T(f'(f(x))) = \frac{1}{1 + \tilde{\epsilon}} \cdot \tilde{c}(x)$.

  We have now shown that the stated reduction is gap-introducing; it remains to show that it is also enhanced.
  Suppose $y_T \in S_T(f'(f(x)))$ and $m_T(f'(f(x)), y_T) \geq \frac{1}{1 + \tilde{\epsilon}} \cdot \tilde{c}(x) = \frac{1}{1 + \beta} \cdot c_T(f'(f(x)))$.
  Our goal is to show that $(x, \tilde{g}(x, y_T)) \in R_P$ if and only if $x \in P$.
  Since we have an enhanced gap-preserving reduction from $Q$ to $T$, we know that $g'(f(x), y_T) \in S_Q(f(x))$ and $m_Q(f(x), g'(f(x), y_T)) \geq \frac{1}{1 + \alpha} \cdot c_Q(f(x))$.
  Since we have an enhanced gap-introducing reduction from $P$ to $Q$ and $\frac{1}{1 + \alpha} \cdot c_Q(f(x)) \geq \frac{1}{1 + \epsilon} \cdot c(x)$ by hypothesis, we know that $(x, g(x, g'(f(x), y_T))) \in R_P$ if and only if $x \in P$.
  Hence $(x, \tilde{g}(x, y_T)) \in R_P$ if and only if $(x, g(x, g'(f(x), y_T))) \in R_P$ if and only if $x \in P$.

  Since we have proven the three conditions required for an enhanced gap-introducing reduction, we finally conclude that $(\tilde{f}, \tilde{g}, \tilde{c}, \tilde{\epsilon})$ is an enhanced gap-introducing reduction from $P$ to $T$.
  The functions $\tilde{f}$, $\tilde{g}$, and $\tilde{c}$ are all polynomial time (respectively, \NC) computable because polynomial time (respectively, \NC) computable functions compose.
\end{proof}

\section{Hardness of approximation}\label{sec:hardness}

In this section we use the PCP theorem to prove \textsc{Maximum 3-Satisfiability} is hard to approximate (specifically, that for certain constant factors, no approximation can exist unless there is some collapse among complexity classes).
We do this by reducing \textsc{Satisfiability} to \textsc{Maximum $k$-Function Satisfiability}, and then reducing that to \textsc{Maximum 3-Satisfiability}, ensuring that the reductions compose.
We first prove that the problem is hard to approximate in polynomial time, then translate the proof to the efficient and highly parallel setting to show that the problem is also hard to approximate in parallel.

\begin{definition}[\textsc{$k$-Function Satisfiability}]
  \mbox{}
  \begin{instance}
    finite set of Boolean variables $x_1, x_2, \ldots, x_n$ and a finite set of  functions $g_1, g_2, \ldots, g_m$, each of which is a function of $k$ of the variables, where $k \geq 2$
  \end{instance}
  \begin{question}
    Does there exist a truth assignment $\tau$ such that all the functions are satisfied?
  \end{question}
\end{definition}

\begin{definition}[\textsc{Maximum $k$-Function Satisfiability}]
  \mbox{}
  \begin{instance}
    finite set of Boolean variables $x_1, x_2, \ldots, x_n$ and a finite set of functions $g_1, g_2, \ldots, g_m$, each of which is a function of $k$ of the variables, where $k \geq 2$
  \end{instance}
  \begin{solution}
    a truth assignment $\tau$ to the variables
  \end{solution}
  \begin{measure}
   the number of satisfied functions
  \end{measure}
\end{definition}

\begin{lemma}
  $\textsc{Maximum \textit{k}-Function Satisfiability} \in \NNCO$, and more generally in $\NPO$.
\end{lemma}
\begin{proof}
  The proof is similar to the proof of \autoref{lem:maxsatnnco}.
\end{proof}

\subsection{Hardness of polynomial time approximation}\label{ssc:phardness}

The main tool of this section is the following theorem, which provides sufficient conditions for an optimization problem in \NPO{} to be hard to approximate.

\begin{theorem}[{\cite[Theorem~3.7]{book}}]\label{thm:gap}
  Let $P$ be an \NP-complete decision problem and $Q$ be a maximization problem in \NPO.
  If there is a polynomial time gap-introducing reduction from $P$ to $Q$ with gap parameter $\epsilon$, then for all $r < 1 + \epsilon$, there is no polynomial time $r$-approximator for $P$ unless $\P = \NP$.
\end{theorem}
\begin{proof}
  In order to produce a contradiction, suppose that there exists a polynomial time $r$-approximator $A$ for $Q$ with $r < 1 + \epsilon$.
  Let $f$ and $c$ be the functions which define the gap-introducing reduction.
  Define algorithm $A'$ as follows on input $x$: $A'(x)$ accepts if and only if $m(f(x), A(f(x))) > \frac{1}{1 + \epsilon} \cdot c(x)$, for all $x\in\Sigma^*$.
  We will show that $A'$ is a polynomial time algorithm for the \NP-complete problem $P$, and hence $\P=\NP$.

  Since $f$, $A$, $c$, and basic arithmetic operations such as addition and multiplication are polynomial time computable functions, so is $A'$.
  In order to show that $A'$ is correct we must consider two cases.
  \begin{enumerate}
  \item
    Suppose $x \in P$.
    Since $A$ is an $r$-approximator for a maximization problem and $r < 1 + \epsilon$, we have
    \begin{displaymath}
      \frac{m^*(f(x))}{m(f(x), A(f(x)))} \leq r < 1 + \epsilon,
    \end{displaymath}
    which implies
    \begin{displaymath}
      \frac{m(f(x), A(f(x)))}{m^*(f(x))} \geq \frac{1}{r} > \frac{1}{1 + \epsilon}.
    \end{displaymath}
    Since $m^*(f(x)) \geq c(x)$ by hypothesis, this implies
    \begin{align*}
      m(f(x), A(f(x))) & > \frac{1}{1 + \epsilon} \cdot m^*(f(x)) \\
      & \geq \frac{1}{1 + \epsilon} \cdot c(x).
    \end{align*}
    Hence $A'$ will accept on input $x$.
  \item
    If $x \notin P$ then $m^*(f(x)) < \frac{1}{1 + \epsilon} \cdot c(x)$ by hypothesis.
    Since $Q$ is a maximization problem, $m(f(x), A(f(x))) \leq m^*(f(x))$.
    Combining the two inequalities, we find $m(f(x), A(f(x))) < \frac{1}{1 + \epsilon} \cdot c(x)$.
    Hence, $A'$ will reject on input $x$.
  \end{enumerate}
  We have shown that $A'$ is a correct polynomial-time computable algorithm which decides the \NP-complete problem $P$, and the conclusion follows.
\end{proof}

\begin{corollary}\label{cor:notinptas}
  Let $P$ be an \NP-complete decision problem and $Q$ be a maximization problem in \NPO.
  If there is a polynomial time gap-introducing reduction from $P$ to $Q$ with gap parameter $\epsilon$ then $Q \notin \PTAS$ unless $\P = \NP$.
\end{corollary}
\begin{proof}
  In order to produce a contradiction, suppose that there exists a polynomial time approximation scheme for $Q$, so there exists a function $f$ such that on input $x$ and $N$, $f$ produces solutions for $Q$, $f$ is computable in time polynomial in the length of $x$, and $R(x, f(x, N)) \leq 1 + \frac{1}{N}$ for all $x \in \Sigma^*$ and all $N \in \mathbb{N}$.
  Specifically, if we choose $N > \frac{1}{\epsilon}$ then $R(x, f(x, N)) \leq 1 + \frac{1}{N} < 1 + \epsilon$, for all $x \in \Sigma^*$.
  Therefore, $f$ is in fact a polynomial time $r$-approximator for $Q$, where $r < 1 + \epsilon$.
  The conclusion follows from \autoref{thm:hard}.
\end{proof}

In order to show that \textsc{Maximum 3-Satisfiability} is hard to approximate, we will show a reduction to it from \textsc{Satisfiability} using \textsc{Maximum $k$-Function Satisfiability} as an intermediate maximization problem.

\begin{lemma}[{\cite[Lemma~29.10]{vazirani}}]\label{lem:intro}
  There is an \NC{} (and more generally, a polynomial time) enhanced gap-introducing reduction from \textsc{Satisfiability} to \textsc{Maximum $k$-Function Satisfiability} with gap parameter $1$, for some $k \in \mathbb{N}$.
\end{lemma}
\begin{proof}
  For the sake of brevity, we will let $P = \textsc{Satisfiability}$ and $Q = \textsc{Maximum \textit{k}-Function Satisfiability}$.

  By \autoref{thm:pcp}, there exists a $(\lg n, 1)$-verifier for $P$.
  Let $\phi$ be a Boolean formula of length $n$, which will be the input to the verifier.
  Let $V$ be that verifier, let $c_0$ and $q$ be the constants such that $V$ uses at most $c_0 \lg n$ random bits and at most $q$ bits of the proof.
  For simplicity, let the length of the random string input to $V$ be exactly $c_0 \lg n$.
  When considering all possible random strings $\rho$ from which $V$ reads its random bits, $V$ reads a total of at most $q \cdot 2^{c_0 \lg n}$ bits of the proof, which equals $q \cdot n^{c_0}$.
  Let $B$ be the set of at most $q \cdot n^{c_0}$ Boolean variables representing the values of the bits of the proof at the queried locations.

  We let $k = q$, and then for each string $\rho$ (of length $c_0 \lg n$), we will define a function $g_\rho$ which is a function of at most $q$ of the Boolean variables from $B$.
  Consider the Cook-Levin transformation of the operation of the verifier $V$ on input $(\phi, \tau; \rho)$, where $\phi$ is a Boolean formula (an instance of $P$) and $\tau$ is a satisfying assignment to the variables of $\phi$.
  Let $\psi$ be the Boolean formula over the variables $z_1, z_2, \ldots, z_{h(n)}$ produced by this transformation, where $h(n)$ is the polynomial bounding the size of the Boolean formula produced by the Cook-Levin transformation.
  Some of these variables depend on the bits of $\phi$, some depend on $q$ bits of $\tau$, and some depend on the bits of $\rho$ (and these sets may intersect).
  If we let $\phi$ and $\rho$ be fixed, however, we can define $g_\rho$ to be the restriction of the Boolean function $\psi$ to the $q$ variables of $\psi$ corresponding to the $q$ bits of $\tau$ read by the verifier on input $(\phi, \tau; \rho)$.

  Let $G = \left\{g_\rho \,\middle|\, \rho \in \Sigma^* \textnormal{ and } |\rho| = c_0 \lg n \right\}$.
  Notice that $|G| = 2^{c_0 \lg n} = n^{c_0}$.
  Now we define the enhanced gap-introducing reduction as follows for all Boolean formulae $\phi$ of length $n$, and all truth assignments $\tau$ to the variables in $B$ (\emph{not} the variables of $\phi$):
  \begin{align*}
    f(\phi) & = (B, G) \\
    c(\phi) & = n^{c_0} \\
    \epsilon & = 1 \\
    g(\phi, \tau) & = \tau^+
  \end{align*}
  where $\tau^+$ is the extension of $\tau$ in which any variables in $\phi$ not in $B$ are assigned an arbitrary binary value (say 0).
  The function $c$ is \NC{} computable because multiplication is in \NC{}, and there are a constant number of multiplications.
  The function $f$ is \NC{} computable because $B$ is simply a list of variables which requires almost no computation and the functions in $G$ can be constructed by $n^{c_0}$ processors in parallel, each of which constructs the function $g_\rho$ by a \ldots.
  \begin{todo}
    How is each function $g_\rho$ in $G$ constructed?
  \end{todo}
  The function $g$ is \NC{} computable because it simply copies $\tau$ to its output along with a polynomial number of extra bits.

  If $\phi \in P$ then there is a truth assignment $\tau$ such that $V$ accepts on input $(\phi, \tau; \rho)$ with probability 1 over the random strings $\rho$.
  In this case, $m^*(f(\phi)) = m^*((B, G)) = n^{c_0}$, since all the functions $g_\rho$ in $G$ are satisfied.
  If $\phi \notin P$ then for every truth assignment, the verifier accepts with probability at most $\frac{1}{2}$.
  In this case, every truth assignment satisfies less than half of all the $n^{c_0}$ functions in $G$.
  Hence $m^*(f(\phi)) = m^*((B, G)) < \frac{1}{2} \cdot n^{c_0} = \frac{1}{1 + \epsilon} c(\phi)$.
  
  To show that this gap-introducing reduction is enhanced, we suppose $\tau$ is a satisfying assignment for $f(\phi)$ which satisfies at least $\frac{1}{2} \cdot n^{c_0}$ of the functions in $f(\phi)$.
  Let $R_P$ be the \NP-relation corresponding to $P$.
  Our goal is to show that $(\phi, g(\phi, \tau)) \in R_P$ if and only if $\phi \in P$.
  By \autoref{lem:xinp}, it suffices to show that $\phi \in P$ implies $(\phi, g(\phi, \tau)) \in R_P$.
  If $\phi$ is satisfiable, then all its clauses are satisfiable.
  Subsequently, all the functions in $f(\phi)$ are satisfiable.
  Then $g(\phi, \tau)$, which equals $\tau^+$, is a truth assignment on which the verifier $V$ will accept with probability 1 (it ignores the bits of $\tau^+$ not in $\tau$).
  Hence $(\phi, g(\phi, \tau)) \in R_P$.

  Therefore we have constructed an \NC{} enhanced gap-introducing reduction with gap parameter 1 from $P$ to $Q$ (that is, from \textsc{Satisfiability} to \textsc{Maximum $k$-Function Satisfiability}).
\end{proof}

\begin{lemma}[{\cite[Proof of Theorem~29.7]{vazirani}}]\label{lem:decision}
  There is an \NC{} many-one reduction from \textsc{$k$-Function Satisfiability} to \textsc{3-Satisfiability}.
\end{lemma}
\begin{proof}
  Let $(\{x_1, x_2, \ldots, x_n\}, \{f_1, f_2, \ldots, f_m\})$ be an instance of \textsc{$k$-Function Satisfiability}.
  Without loss of generality, each function $f_i$ of $k$ variables can be written as a Boolean formula containing at most $2^k$ clauses, in which each clause contains at most $k$ literals (why?).
  Call this Boolean formula $\psi_i$, and let $\psi = \bigwedge_{i = 1}^m{\psi_i}$.
  Observe that the given instance of \textsc{$k$-Function Satisfiability} is satisfiable if and only if $\psi$ is satisfiable.

  Let $T$ be the \NC{} computable function from \autoref{lem:three}.
  Now we can define the reduction $g$ by $g(\phi) = T(\psi)$ for all Boolean formulae $\phi$.
  The total number of clauses in $\psi$ is at most $m \cdot 2 ^ k$, so the total number of clauses in $T(\psi)$ is at most $m \cdot 2^k \cdot (k - 2)$ (still a polynomial in the length of the input since $k$ is considered a fixed constant).
  $g$ is \NC{} computable because each of the $m$ functions can be transformed by an independent processor, each processor performs the transformation from $f_i$ to $\psi_i$ in a constant amount of time, and for each processor the transformation fron $\psi_i$ to $T(\psi_i)$ is computable in \NC.
  \begin{todo}
    Check that the transformation from $f_i$ to $\psi_i$ is computable in \NC.
  \end{todo}
  Finally, $\phi$ is satisfiable if and only if $\psi$ is satisfiable if and only if $T(\psi)$ is satisfiable.
\end{proof}

\begin{lemma}\label{lem:opt}
  There exists an \NC{} (and more generally, a polynomial time) enhanced gap-preserving reduction from \textsc{Maximum $k$-Function Satisfiability} to \textsc{Maximum 3-Satisfiability}.
\end{lemma}
\begin{proof}
  For brevity, we let $P = \textsc{Maximum \textit{k}-Function Satisfiability}$ and $Q = \textsc{Maximum 3-Satisfiability}$.

  We assume $k$ is a fixed constant.
  Let $f_0$ be the reduction given in \autoref{lem:decision} from \textsc{$k$-Function Satisfiability} to \textsc{3-Satisfiability}.
  Define the six functions as follows for all finite sets of Boolean variables $U$, all finite sets of Boolean functions $F$ of at most $k$ of the variables in $U$, and all truth assignments $\tau$ to 3-CNF Boolean formulae:
  \begin{align*}
    f(\pair{U}{F}) & = f_0(\pair{U}{F}) \\
    g(\pair{U}{F}, \tau) & = \tau|_U \\
    c_P(\pair{U}{F}) & = |F| \\
    c_Q(\phi) & = |\phi| \\
    \alpha & = 1 \\
    \beta & = 2^{k + 1} \cdot (k - 2) - 1 \qquad \text{(see explanation below)},
  \end{align*}
  where $\tau|_U$ represents the restriction of the satisfying truth assignment $\tau$ to only the variables of $U$.
  Intuitively, the definitions of $c_P$ and $c_Q$ suggest that if all functions in $F$ are satisfiable then all clauses in $f(\pair{U}{F})$ will be satisfiable (there are at most $M \cdot 2^k \cdot (k - 2)$ of them, from the proof of \autoref{lem:decision}), and the definitions of $\alpha$ and $\beta$ suggest \ldots
  \begin{todo}
    What do the definitions of $\alpha$ and $\beta$ suggest, intuitively?
  \end{todo}

  $f$ is computable by an \NC{} circuit family because $f_0$ is.
  $g$ is computable by an \NC{} circuit family because it simply chooses a subset of the bits of $\tau$.
  $c_P$ and $c_Q$ are computable in \NC{} because they simply output the length of their respective inputs (or part of their inputs).

  Suppose $m^*_P(\pair{U}{F}) \geq c_P(\pair{U}{F}) = |F|$, which implies that all the functions in $F$ are satisfiable.
  By the correctness of the many-one reduction $f_0$, all functions in $F$ are satisfiable if and only if all clauses in $f_0(\pair{U}{F})$ are satisfiable.
  Therefore, if $\phi$ denotes $f_0(\pair{U}{F})$, then $m^*_Q(f(\pair{U}{F})) = m^*_Q(\phi) = |\phi| = c_Q(\phi)$.

  Now suppose instead that $m^*_P(\pair{U}{F}) < \frac{1}{2} \cdot |F|$, so less than half of the functions in $F$ are satisfiable, or in other words, at least half of the functions in $F$ are unsatisfiable.
  Suppose $\gamma_i$ is the conjunction which represents $f_i$ in the formula $f_0(\pair{U}{F})$.
  For each function $f_i$ which is not satisfiable, at least one clause of $\gamma_i$ must be unsatisfiable, by the correctness of $f_0$.
  Hence the number of unsatisfiable clauses in $f(\pair{U}{F})$ is at least $\frac{1}{2}|F|$, so $m^*_Q(f(\pair{U}{F})) < \frac{1}{2} |F|$.
  Since $c_Q(f(\pair{U}{F})) = |f_0(\pair{U}{F})| \leq |F| \cdot 2^k \cdot (k - 2)$, we must choose $\beta$ such that $\frac{1}{2} |F| \leq \frac{1}{1 + \beta} \cdot |F| \cdot 2^k \cdot (k - 2)$.
  Hence, we choose $\beta = 2^{k + 1} \cdot (k - 2) - 1$.

  Finally, to show that this gap-introducing reduction is enhanced, we suppose $\pair{U}{F}$ is an instance of \textsc{Maximum $k$-Function Satisfiability}, and let $\phi = f(\pair{U}{F})$.
  Furthermore, suppose $\tau$ is a truth assignment to $\tau$ which satisfies at least $\frac{1}{1 + ???}$ of the maximum number of satisfiable clauses in $\phi$.
  By definition, $g(\pair{U}{F}, \tau) = \tau|_U$, so it is certainly a valid solution to $\pair{U}{F}$.
  Our goal is to show that $m(\pair{U}{F}, \tau|_U) \geq \frac{1}{2} \cdot |F|$.
  Again, by the correctness of the many-one reduction $f_0$, each function in $F$ is satisfiable if and only if the corresponding conjunction of clauses in $f(\pair{U}{F})$ are satisfiable.
  In other words, for each conjunction of (at most $k - 2$) related (in the sense that they derive from the same single clause in $F$) clauses in $f(\pair{U}{F})$ which is satisfiable, the single clause in $F$ from which that collection derived is satisfiable, and the converse is true, as well.
  \begin{todo}
    Complete the ``enhanced'' part of this proof.
  \end{todo}
\end{proof}

\begin{lemma}\label{lem:gap3}
  There is an \NC{} (more generally, a polynomial time) enhanced gap-introducing reduction from \textsc{Satisfiability} to \textsc{Maximum 3-Satisfiability}.
\end{lemma}
\begin{proof}
  We will use \autoref{lem:intro} and \autoref{lem:opt} to invoke \autoref{lem:compose}.

  We will let $(f, g, c, \epsilon)$ be the \NC{} enhanced gap-introducing reduction and let $(f', g', c_Q, c_T, \alpha, \beta)$ be the \NC{} enhanced gap-preserving reduction.
  The enhanced gap-introducing reduction of \autoref{lem:intro} has
  \begin{align*}
    c(\phi) & = n^{c_0}, \text{ and} \\
    \epsilon & = 1,
  \end{align*}
  where $n = |\phi|$ and $c_0$ is a constant which comes from the proof of \autoref{lem:intro} (specifically, from bounding the number of random bits read by the verifier).
  The enhanced gap-preserving reduction of \autoref{lem:opt} has
  \begin{align*}
    c_Q(\pair{U}{F}) & = |F|, \\
    c_T(\phi) & = |\phi| \\
    \alpha & = 1, \text{ and} \\
    \beta & = 2^{k + 1} \cdot (k - 2) - 1,
  \end{align*}
  where $k$ is some constant.

  To meet the conditions in the hypothesis of \autoref{lem:compose}, we need $c(\phi) \geq c_Q(f(\phi))$ and $\frac{1}{1 + \epsilon} \cdot c(\phi) \leq \frac{1}{1 + \alpha} \cdot c_Q(f(\phi))$.
  Since $c(\phi) = n^{c_0}$ and $c_Q(f(\phi)) = c_Q(\pair{B}{G}) = |G| = n^{c_0}$, the first inequality is satisfied.
  Since $\frac{1}{1 + \epsilon} \cdot c(\phi) = \frac{1}{2} \cdot n^{c_0}$ and $\frac{1}{1 + \alpha} \cdot c_Q(f(\phi)) = \frac{1}{2} \cdot c_Q(\pair{B}{G}) = \frac{1}{2} \cdot n^{c_0}$, the second inequality is also satisfied.

  Therefore we can conclude that there is an \NC{} enhanced gap-introducing reduction $(\tilde{f}, \tilde{g}, \tilde{c}, \tilde{\epsilon})$ from \textsc{Satisfiability} to \textsc{Maximum 3-Satisfiability}, where
  \begin{align*}
    \tilde{f}(\phi) & = f'(f(\phi)) \\
    & = f'(\pair{B}{G}) \\
    & = f_0(\pair{B}{G}), \\
    \tilde{g}(\phi, \tau) & = g(\phi, g'(f(\phi), \tau)) \\
    & = g(\phi, g'(\pair{B}{G}, \tau)) \\
    & = g(\phi, \tau|_B) \\
    & = {\tau|_B}^+, \\
    \tilde{c}(\phi) & = c_T(f'(f(\phi))) \\
    & = c_T(f'(\pair{B}{G})) \\
    & = c_T(f_0(\pair{B}{G})) \\
    & = |f_0(\pair{B}{G})| \\
    & = |G| \cdot 2^k \cdot (k - 2), \text{ and} \\
    \tilde{\epsilon} & = \beta \\
    & = 2^{k + 1} \cdot (k - 2) - 1. \qedhere
  \end{align*}
\end{proof}

Although \autoref{lem:gap3} specifies an enhanced gap-introducing reduction, the following theorem does not require that the reduction be enhanced.
However, we provide the enhanced reduction because \autoref{lem:gap3} will be used in \autoref{sec:complete}, in order to prove that \textsc{Maximum 3-Satisfiability} is complete for the class \APX (and \NCX).

\begin{theorem}[{\cite[Theorem~6.3]{book}} and {\cite[Corollary~29.8]{vazirani}}]\label{thm:hard}
  There is a constant $k$ such that no polynomial time $r$-approximator for \textsc{Maximum 3-Satisfiability} exists unless $\P = \NP$, for all $r < 2^{k + 1} \cdot (k - 2)$.
\end{theorem}
\begin{proof}
  Follows from \autoref{lem:gap3} and \autoref{thm:gap}.
\end{proof}

\begin{todo}
  This should be something like $r < 1 + \frac{1}{2^{k + 1} \cdot (k - 2) - 1}$ \ldots
\end{todo}

\begin{corollary}\label{cor:satnotinptas}
  $\textsc{Maximum 3-Satisfiability} \notin \PTAS$ unless $\P = \NP$.
\end{corollary}
\begin{proof}
  Follows from \autoref{thm:hard} and \autoref{cor:notinptas}.
\end{proof}

\subsection{Hardness of parallel approximation}\label{ssc:nchardness}

\begin{theorem}[\NC{} version of \autoref{thm:gap}]\label{thm:ncgap}
  Let $P$ be a decision problem which is complete for \NNC{} under \NC{} many-one reductions.
  Let $Q$ be a maximization problem in \NNCO.
  If there is an \NC{} gap-introducing reduction from $P$ to $Q$ with gap parameter $\epsilon$, then for all $r < 1 + \epsilon$, there is no \FNC{} $r$-approximator for $P$ unless $\NC = \NNC$.
\end{theorem}
\begin{proof}
  The proof is similar to the proof of \autoref{thm:gap}.
\end{proof}

\begin{corollary}\label{cor:notinncas}
  Let $P$ be a decision problem which is complete for \NNC{} under \NC{} many-one reductions.
  Let $Q$ be a maximization problem in \NNCO.
  If there is an \NC{} gap-introducing reduction from $P$ to $Q$ then $Q \notin \NCAS$ unless $\NC = \NNC$.
\end{corollary}
\begin{proof}
  The proof is similar to the proof of \autoref{cor:notinptas}.
\end{proof}

By \autoref{lem:gap3}, we already know that there exists an \NC{} enhanced gap-introducing reduction from \textsc{Satisfiability} to \textsc{Maximum 3-Satisfiability}.
By \autoref{lem:satisnnccomplete} we know that \textsc{Satisfiability} is complete for \NNC{} under \NC{} many-one reductions, and by \autoref{lem:maxsatnnco} we know that $\textsc{Maximum 3-Satisfiability}$ is in \NNCO.
Therefore we have the following analogs of \autoref{thm:hard} and \autoref{cor:satnotinptas}.

\begin{theorem}\label{thm:nchard}
  There is a constant $k$ such that no \FNC{} $r$-approximator for \textsc{Maximum 3-Satisfiability} exists unless $\NC = \NNC$, for all $r < 2^{k + 1} \cdot (k - 2)$.
\end{theorem}

\begin{corollary}
  $\textsc{Maximum 3-Satisfiability} \notin \NCAS$ unless $\NC = \NNC$.
\end{corollary}

\section{Completeness in approximation classes}\label{sec:complete}

We can now give another perspective on why \textsc{Maximum 3-Satisfiability} has no polynomial time approximation scheme unless $\P = \NP$, and why it has no \NC{} approximation scheme unless $\NC = \NNC$.
In this section we prove that this problem is in fact complete for \APX{} under polynomial time AP reductions, and complete for \NCX{} under \NC{} AP reductions.
This means that if there were a polynomial time approximation scheme for this problem, then there would be one for all problems in \APX, and if there were an \NC{} approximation scheme for it, then there would be one for all problems in \NCX.

\begin{todo}
  What does it mean for an optimization problem to be both complete for \APX{} under polynomial time reductions and complete for \NCX{} under polynomial time reductions?
\end{todo}

\begin{todo}
  Find a reference for the following statement (probably in the Ausiello book): if $P$ is \APX-complete and $P \in \PTAS$ then $\P = \NP$.
  Hence (we should note), \APX-completeness implies hardness of approximation.
  The key component which implies both $P \notin \PTAS$ and $P$ is \APX-complete is the gap-introducing reduction.
\end{todo}

\subsection{\texorpdfstring{\APX}{APX}-completeness}\label{ssc:apxcomplete}

To show that \textsc{Maximum 3-Satisfiability} is complete for \APX{} under AP reductions we must show that it is in \APX{} and that there is an AP reduction from every optimization problem in \APX{} to it.
We will do this in three steps.
First, we will show that it is in \APX.
Second, we will show that it is complete under AP reductions for the class of maximization problems in \APX.
Finally, we will show that there is an AP reduction from every minimization problem to some maximization problem in \APX.

\begin{theorem}\label{thm:inncx}\label{thm:inapx}
  \textsc{Maximum 3-Satisfiability} is in \NCX, and more generally in \APX.
\end{theorem}
\begin{proof}
  \textsc{Maximum 3-Satisfiability} is a restriction of \textsc{Maximum Satisfiability}, and there exists a \FNC{} 2-approximator for \textsc{Maximum Satisfiability} (see \cite[Program~3.1]{book}).
  \begin{todo}
    Examine this algorithm to see if it is in \NC.
    Otherwise, look at \cite{dsst97} to see if they have an algorithm for \textsc{Maximum 3-Satisfiability}.
  \end{todo}
  Since $\NCX \subseteq \APX$, it is also in \APX.
\end{proof}

Before proceeding to \autoref{thm:maxcomplete}, we require the following lemmata.

\begin{lemma}\label{lem:interval}
  Let $P \in \APX$, with $P = (I, S, m, \max)$, and let $A$ be the $r$-approximator for $P$ for some constant $r$.
  Let $r_n \geq 1$.
  There exists a nondeterministic polynomial time algorithm $B$ such that on input $(x, i)$, where $x \in I$ and $i \in \mathbb{N}$, $B$ decides if there exists a solution $y$ with
  \begin{equation*}
    {r_n}^i \cdot m(x, A(x)) \leq m(x, y) \leq {r_n}^{i + 1} \cdot m(x, A(x)).
  \end{equation*}
  Furthermore, if a solution $y$ exists, $B$ writes $y$ before accepting.
\end{lemma}
\begin{proof}
  Let $p$ be the polynomial which bounds the size of the solutions for an instance $x$ of size $n$.
  Define $B$ as in \autoref{alg:interval}.
  \begin{algorithm}
    \caption{Nondeterministic polynomial time algorithm that decides if there is an approximate solution for instance $x$ of problem $P$ in interval $i$%
      \label{alg:interval}}
    \begin{algorithmic}[1]
      \Require{$x \in I$, $i \geq 0$}
      \Statex{}
      \Function{$B$}{$x$, $i$}:
        \State{\textbf{guess} $y$ with $|y| \leq p(|x|)$}
        \State{$a \gets m_P(x, A_P(x))$}
        \If{$(x, y) \in S$ and ${r_n}^i \cdot a \leq m_P(x, y) < {r_n}^{i + 1} \cdot a$}
          \State{\textbf{write} $y$}
          \State{\textbf{Accept}}
        \Else
          \State{\textbf{Reject}}
        \EndIf
      \EndFunction
    \end{algorithmic}
  \end{algorithm}
  Since all operations used during the execution of $B$ are polynomial time computable, $B$ runs in polynomial time.
  Since $B$ accepts if and only if there exists a solution $y$ with the stated bounds, the conclusion follows.
\end{proof}

\begin{lemma}\label{lem:bounds}
  Let $r, r_i, k > 1$.
  Let $\psi$ be a Boolean formula which satisfies $\psi = \psi_0 \land \psi_1 \land \dotsb \land \psi_{k - 1}$, where each $\psi_i$ is a 3-CNF Boolean formula with at most $M$ satisfiable clauses.
  Let $R(\psi, \tau)$ be the performance ratio corresponding to \textsc{Maximum 3-Satisfiability} for all 3-CNF Boolean formulae $\psi$ and all truth assignments $\tau$.
  Let $\tau$ be a truth assignment such that $R(\psi, \tau) \leq r$ and $R(\psi_i, \tau) = r_i$, for all $i \leq k - 1$ (here, $R(\psi_i, \tau)$ is the performance ratio of $\tau$ with respect to only $\psi_i$).
  Then $r_i \leq \frac{1}{1 - 2k\left(\frac{r - 1}{r}\right)}$.
\end{lemma}
\begin{proof}
  Let $m$ be the measure for \textsc{Maximum 3-Satisfiability}, so $m(\psi, \tau)$ is the number of clauses satisfied by $\tau$.
  First, we have
  \begin{align*}
    & \phantom{\implies} R(\psi, \tau) = \frac{m^*(\psi)}{m(\psi, \tau)} \leq r \\
    & \implies m(\psi, \tau) \geq \frac{m^*(\psi)}{r}. \\
  \end{align*}
  From this, we have
  \begin{align*}
    m^*(\psi) - m(\psi, \tau) & \leq m^*(\psi) - \frac{m^*(\psi)}{r} \\
    & = m^*(\psi) \left(1 - \frac{1}{r}\right) \\
    & = m^*(\psi) \left(\frac{r - 1}{r}\right)\\
    & \leq kM \left(\frac{r - 1}{r}\right),
  \end{align*}
  where the final inequality is true because there are at most $M$ satisfiable clauses in each of the $k$ formulas $\psi_0, \psi_1, \dotsc, \psi_{k - 1}$.

  Since
  \begin{equation*}
    m^*(\psi) = \sum_{i = 0}^{k - 1}{m^*(\psi_i)} \qquad \text{and} \qquad m(\psi, \tau) = \sum_{i = 0}^{k - 1}{m(\psi_i, \tau)},
  \end{equation*}
  we have
  \begin{align*}
    m^*(\psi) - m(\psi, \tau) & = \sum_{j = 0}^{k - 1}{m^*(\psi_j)} - \sum_{j = 0}^{k - 1}{m(\psi_j, \tau)} \\
    & = \sum_{j = 0}^{k - 1}{\left(m^*(\psi_j) - m(\psi_j, \tau)\right)} \\
    & \geq m^*(\psi_i) - m(\psi_i, \tau) \\
    & = m^*(\psi_i) \left(\frac{r_i - 1}{r_i}\right) \\
    & \geq \frac{M}{2} \cdot \frac{r_i - 1}{r_i},
  \end{align*}
  where the last inequality follows from \autoref{lem:half}.
  Combining the upper and lower bounds, we have
  \begin{equation*}
    \frac{M}{2} \cdot \frac{r_i - 1}{r_i} \leq kM \left(\frac{r - 1}{r}\right),
  \end{equation*}
  from which we find
  \begin{equation*}
    1 - \frac{1}{r_i} \leq 2k\left(\frac{r - 1}{r}\right),
  \end{equation*}
  and hence
  \begin{equation*}
    1 - 2k\left(\frac{r - 1}{r}\right) \leq \frac{1}{r_i}.
  \end{equation*}
  Taking the multiplicative inverse, we find
  \begin{equation*}
    r_i \leq \frac{1}{1 - 2k\left(\frac{r - 1}{r}\right)},
  \end{equation*}
  which concludes the proof.
\end{proof}

\begin{lemma}\label{lem:alpha}
  Let $r_P, \epsilon \in \mathbb{Q}^+$ with $r_P > 1$ and $\epsilon > 0$.
  There exists a constant $\alpha \in \mathbb{Q}^+$ (which depends only on $r_P$ and $\epsilon$) with $\alpha > 1$ such that the following is true.
  Let $r \in \mathbb{Q}^+$ with $r > 1$, let $r_n = 1 + \alpha \cdot (r - 1)$, and let $k = \log_{r_n}{r_P}$.
  Then
  \begin{equation*}
    r < 1 + \frac{\epsilon}{2k \cdot (1 + \epsilon) - \epsilon}.
  \end{equation*}
\end{lemma}
\begin{proof}
  Since $r_n = 1 + \alpha \cdot (r - 1)$, we have $r = 1 + \frac{r_n - 1}{\alpha}$, so we intend to show
  \begin{equation*}
    1 + \frac{r_n - 1}{\alpha} < 1 + \frac{\epsilon}{2k \cdot (1 + \epsilon) - \epsilon},
  \end{equation*}
  or in other words,
  \begin{equation*}
    \alpha > \frac{(r_n - 1) \left[2k \cdot (1 + \epsilon) - \epsilon\right]}{\epsilon}.
  \end{equation*}
  In fact, we will prove the stronger lower bound
  \begin{equation*}
    \alpha > \frac{(r_n - 1)\left[2k \cdot (1 + \epsilon)\right]}{\epsilon}.
  \end{equation*}

  Since $k$ is defined to equal $\ceil{\log_{r_n}{r_P}}$, we have
  \begin{align*}
    k & \leq 1 + \frac{\log{r_P}}{\log{r_n}} \\
    & \leq 1 + \frac{r_n \log{r_P}}{\log{r_n}} \\
    & \leq 1 + \frac{r_n \log{r_P}}{r_n - 1} \\
    & = \frac{r_n \log{r_P} + r_n - 1}{r_n - 1} \\
    & < \frac{r_P \log{r_P} + r_P - 1}{r_n - 1},
  \end{align*}
  where the last inequality is true because we have assumed $r_n < r_P$.
  Since we know $k \cdot (r_n - 1) < r_P \log r_P + r_P - 1$ it suffices to choose
  \begin{equation*}
    \alpha \geq \frac{2(1 + \epsilon)(r_P \log r_P + r_P - 1)}{\epsilon}.
  \end{equation*}
  Choosing $\alpha$ equal to that quantity (so that it depends only on $r_P$ and $\epsilon$) gives us the desired upper bound on $r$.
\end{proof}

\begin{lemma}\label{lem:magic}
  Let $r, r_i, k, \epsilon \in \mathbb{R}^+$ with $r > 1$, $r_i > 1$, $k > 1$, and $\epsilon > 0$.
  If
  \begin{equation*}
    r_i \leq \frac{1}{1 - 2k(\frac{r - 1}{r})} \qquad \text{and} \qquad r < 1 + \frac{\epsilon}{2k \cdot (1 + \epsilon) - \epsilon},
  \end{equation*}
  then $r_i < 1 + \epsilon$.
\end{lemma}
\begin{proof}
  It suffices to show $\frac{1}{1 - 2k(\frac{r - 1}{r})} < 1 + \epsilon$. To determine if $r$ meets this condition, we attempt to express the $r$ that appears in this inequality in terms of $\epsilon$ and $k$.
  \begin{align*}
    & \phantom{\iff} \frac{1}{1 - 2k(\frac{r - 1}{r})} < 1 + \epsilon \\
    & \iff \frac{1}{1 + \epsilon} < 1 - 2k(\frac{r - 1}{r}) \\
    & \iff 2k(\frac{r - 1}{r}) < 1 - \frac{1}{1 + \epsilon} \\
    & \iff \frac{r - 1}{r} < \frac{1}{2k} - \frac{1}{2k \cdot (1 + \epsilon)} \\
    & \iff 1 - \frac{1}{r} < \frac{1 + \epsilon}{2k \cdot (1 + \epsilon)} - \frac{1}{2k \cdot (1 + \epsilon)} \\
    & \iff \frac{1}{r} > 1 - \frac{\epsilon}{2k \cdot (1 + \epsilon)} \\
    & \iff \frac{1}{r} > \frac{2k \cdot (1 + \epsilon)}{2k \cdot (1 + \epsilon)} - \frac{\epsilon}{2k \cdot (1 + \epsilon)} \\
    & \iff r < \frac{2k \cdot (1 + \epsilon)}{2k \cdot (1 + \epsilon) - \epsilon} \\
    & \iff r < 1 + \frac{\epsilon}{2k \cdot (1 + \epsilon) - \epsilon}
  \end{align*}
  Since the last inequality is true by hypothesis and each step is a biconditional, the first inequality is true, which is what we intended to show.
\end{proof}

\begin{theorem}[{\cite[Theorem~8.6]{book}}]\label{thm:maxcomplete}
  \textsc{Maximum 3-Satisfiability} is complete for the class of maximization problems in \APX{} under $\APr$ reductions.
\end{theorem}

We describe the idea of the proof before presenting the proof itself, since it is somewhat technical.

\begin{proofidea}
  First, we recall the enhanced gap-introducing reduction $(f_s, g_s, \epsilon)$ from \autoref{lem:gap3}, which shows that there exist a positive constant $\epsilon$ and functions $f_s$ and $g_s$ such that for any CNF formula $\phi$, if $\tau$ is a truth assignment satisfying at least $\frac{1}{1 + \epsilon}$ of the maximum number of satisfiable clauses of $f_s(\phi)$ then $g_s(\phi, \tau)$ is a satisfying assignment for $\phi$ if and only if $\phi$ is satisfiable.
  Here $f_s$ is a mapping from CNF formulae to 3-CNF formulae.

  The reduction we present is a generic reduction from an arbitrary maximization problem in \APX{} to \textsc{Maximum 3-Satisfiability}, again using a Cook-Levin transformation so that the number of satisfiable clauses in the Boolean formula corresponds in some way to the measure of a solution in the original maximization problem.
  In order to search for a ``good enough'' approximate solution for \textsc{Maximum 3-Satisfiability}, we will partition the interval in which we are certain the optimal solution lives (but which is too large) into a constant number of subintervals.
  For each subinterval we will use the Cook-Levin transformation to construct a Boolean formula which is satisfiable if and only if there is an approximate solution in that subinterval, and which exposes the approximate solution (by having Boolean variables $y_1, y_2, \dotsc, y_k$ whose truth values represent the values of the bits in the binary representation of $y$).
  Furthermore, we apply the function $f_s$ to each of those formulae in order to provide the gap guarantee discussed in the previous paragraph; this allows us to use \ldots.
  Finally, we use $g_s$ on one of the formulae corresponding to a subinterval to produce a ``good enough'' approximate solution for the original maximization problem.
\end{proofidea}
\begin{proof}[Proof of \autoref{thm:maxcomplete}]
  For the sake of brevity, we let the single letter $Q$ represent $\textsc{Maximum 3-Satisfiability}$.

  Let $P$ be a maximization problem in \APX, so we have $P = (I, S, m, \max)$.
  Let $A_P$ be the polynomial time computable $r_P$-approximator for $P$, where $r_P$ is some constant.
  We need to define an AP reduction $(f, g, \alpha)$ from $P$ to $Q$.
  We will delay the definition of the constant $\alpha$ until later in the proof, and until then we assume it has been fixed.
  We now wish to define functions $f$ and $g$ for all $r > 1$.
  Let $r_n = 1 + \alpha \cdot (r - 1)$; this is the desired upper bound of the performance ratio $R_P(x, g(x, \tau, r))$, as required by the AP condition when $R_Q(f(x, r), \tau) \leq r$.

  In the case that $r_p \leq r_n$, defining $g(x, y, r) = A_P(x)$ for all $x$ and $y$ is sufficient, and no definition of $f$ is necessary, since $R(x, g(x, y, r)) = R(x, A_P(x)) \leq r_P \leq r_n$.
  Suppose from here on that $r_P > r_n$.

  For $f$ we intend to map an instance $x$ of $P$ to a 3-CNF Boolean formula, and for $g$ we intend to map a truth assignment which satisfies a constant factor of the satisfiable clauses of $f(x, r)$ to a solution $y$ for $P$.
  In order to recover a solution $y$ for $P$ with $R_P(x, y) \leq r_n$, we need a solution whose measure is close to $m^*_P(x)$, the optimal measure of $x$.
  If we let $a(x) = m_P(x, A_P(x))$, then
  \begin{equation}\label{eq:bounds}
    a(x) \leq m^*_P(x) \leq r_P \cdot a(x),
  \end{equation}
  since $a(x)$ is the measure of a $r_P$-approximate solution to a maximization problem.
  We can't just output any solution in this range, because the approximation guarantee would be only $r_P$; we need it to be $r_n$.
  Hence we will subdivide this interval and derive a solution within a subinterval of size $r_n$.

  Let us divide the interval $[a(x), r_P \cdot a(x)]$ into $k$ subintervals,
  \begin{displaymath}
    \left[a(x), r_n \cdot a(x)\right], \left[r_n \cdot a(x), {r_n}^2 \cdot a(x)\right], \ldots, \left[{r_n}^{k - 1} \cdot a(x), r_P \cdot a(x)\right],
  \end{displaymath}
  where $k = \ceil{\log_{r_n}{r_P}}$.
  (Notice that the sizes of these subintervals are not necessarily equal; except that of the final subinterval, the sizes are only non-decreasing.
  This won't be a problem, as we only need a multiplicative constant approximation, not an additive constant approximation.)
  By construction, we have ${r_n}^k = {r_n}^{\ceil{\log_{r_n}{r_P}}} \geq {r_n}^{\log_{r_n}{r_P}} = r_P$, and hence $r_P \cdot a(x) \leq {r_n}^k \cdot a(x)$.
  Combining this with \autoref{eq:bounds}, we have $a(x) \leq m^*_P(x) \leq r_P \cdot a(x) \leq {r_n}^k \cdot a(x)$, so the optimal measure of $x$ must exist in one of these intervals.
  Now we construct $f$ and $g$ in such a way that for any $x \in I$, we can recover an approximate solution $y$ that lies in the same interval in which $m_P^*(x)$ lies.
  If we were to find a solution $y$ in the same interval in which the optimal measure lies, say, interval $j$, then we would have
  \begin{equation}\label{eq:subinterval}
    {r_n}^j \cdot a(x) \leq m_P(x, y) \leq m^*_P(x) \leq {r_n}^{j + 1} \cdot a(x).
  \end{equation}
  Then, since
  \begin{equation*}
    \frac{m^*_P(x)}{a(x)} \leq {r_n}^{j + 1} \qquad \text{and} \qquad {r_n}^j \leq \frac{m_P(x, y)}{a(x)}
  \end{equation*}
  we find that
  \begin{equation}\label{eq:result}
    R_P(x, y) = \frac{m_P^*(x)}{m_P(x, y)} = \frac{\frac{m^*_P(x)}{a(x)}}{\frac{m_P(x, y)}{a(x)}} \leq \frac{{r_n}^{j + 1}}{{r_n}^j} = r_n.
  \end{equation}
  So this is our goal when constructing $f$ and $g$.

  To define $f$ we need some auxiliary functions.
  Let $B$ be the Turing machine guaranteed by \autoref{lem:interval}.
  Let $f_s$ be the function from the enhanced gap-introducing reduction of \autoref{lem:gap3}.
  Let $C$ be the transformation from \autoref{lem:cooklevin}.
  Now we can define $f$ by
  \begin{displaymath}
    f(x, r) = \bigwedge_{i = 0}^{k - 1}{f_s(C(B, (x, i)))},
  \end{displaymath}
  for all $x \in I$.
  Let us explain this function.
  Since $B$ writes a solution $y$ before it accepts, $C(B, (x, i))$ will have some variables, $y_1, y_2, \dotsc, y_{p(|x|)}$, which, correspond to the bits of a solution $y$, should one exist in subinterval $i$.
  Applying $f_s$ to this Boolean formula produces a 3-CNF formula which has a gap behavior which will be discussed later (since it depends on $g_s$).
  Observe that $f(x, r)$ is still a 3-CNF formula, since a conjunction of 3-CNF formulae is itself a 3-CNF formula.
  We assume without loss of generality also that each formula $f_s(C(B(x, i)))$ has the same number of clauses, say $M$.
  \begin{todo}
    Why is this without loss of generality?
    Adding more clauses may change the fraction of satisfiable clauses\ldots
  \end{todo}

  Now, to determine how to define $g$, first suppose $x \in I$ and $\tau \in S_Q(f(x, r))$.
  For each $i$ with $0 \leq i \leq k - 1$, let $\phi_i = C(B(x, i))$ and $\psi_i = f_s(\phi_i)$.
  Let $\psi = f(x, r)$, so $\tau$ is a truth assignment to $\psi$ which satisfies some of the clauses.
  Suppose $g_s$ is the function from the enhanced gap-introducing reduction which produces solutions for $P$, as described in the preceding discussion.
  For any $\tau$ that satisfies a $\frac{1}{1 + \epsilon}$ factor of the maximum number of satisfiable clauses in $\psi_i$ for all $i \leq k - 1$, if we let $\tau_i = g_s(\phi_i, \tau)$ then $\tau_i$ satisfies $\phi_i$ if and only if $\phi_i$ is satisfiable.
  We will assume for now that $\tau$ does indeed satisfy a $\frac{1}{1 + \epsilon}$ factor of the maximum number of satisfiable clauses of $\psi_i$ for all $i$, and delay the proof of this fact until later.
  Furthermore, if $\phi_i$ is satisfiable via truth assignment $\tau_i$, then we can recover the bits of the solution $y$ from the appropriate variables of $\phi_i$, as described previously.
  Let $T$ be the function which outputs $y$ given $\phi_i$ and $\tau_i$.
  \begin{todo}
    Note that this function can be computed in logarithmic space.
  \end{todo}

  Let $j$ be the maximum $i$ such that $\tau_i$ satisfies $\phi_i$ (such a $j$ exists because $m^*_P(x)$ is in some interval, and for all intervals beyond that one, $\phi_i$ must not be satisfiable).
  This implies that there is a $y$ in interval $j$ with
  \begin{equation*}
    {r_n}^j \cdot a(x) \leq m_P(x, y) \leq m^*_P(x) \leq {r_n}^{j + 1} \cdot a(x),
  \end{equation*}
  which is exactly the statement of \autoref{eq:subinterval}, and hence \autoref{eq:result} follows.
  Therefore, we define $g(x, \tau, r)$ as in \autoref{alg:g}.
  Observe that $g$ is polynomial time computable since $k$, the number of intervals to search, is a constant, and all other functions are polynomial time computable.
  \begin{algorithm}
    \caption{Deterministic polynomial time algorithm that computes a $r_n$-approximate solution for $x$%
      \label{alg:g}}
    \begin{algorithmic}
      \Require{$x \in I$, a truth assignment $\tau$, $r > 1$}
      \Statex{}
      \Function{$g$}{$x$, $\tau$, $r$}:
        \For{$i \in \{0, 1, \dotsc, k - 1\}$}
          \State{$\phi_i = C(B, (x, i))$}
          \State{$\tau_i = g_s(\phi_i, \tau)$}
        \EndFor
        \State{$j \gets \max_{i = 0}^{k - 1}{\left\{i\,\middle|\, \tau_i \text{ satisfies } \phi_i\right\}}$} \\
        \hspace{1.5em}\Return{$T(\phi_j, \tau_j)$}
      \EndFunction
    \end{algorithmic}
  \end{algorithm}

  We now complete the tasks which we delayed above.
  We will prove that any truth assignment $\tau$ to $\psi$ satisfies at least a factor $\frac{1}{1 + \epsilon}$ of the maximum number of satisfiable clauses in each $\psi_i$ for each $i \leq k - 1$, and in doing so we will also choose the value of the constant $\alpha$.
  By \autoref{lem:bounds}, we know that
  \begin{equation*}
    r_i \leq \frac{1}{1 - 2k\left(\frac{r - 1}{r}\right)}.
  \end{equation*}
  By \autoref{lem:alpha} there exists a constant $\alpha$ that depends only on $r_P$ (defined by the approximation algorithm $A_P$) and $\epsilon$ (defined in a previous theorem) such that
  \begin{equation}\label{eq:rupper}
    r < 1 + \frac{\epsilon}{2k \cdot (1 + \epsilon) - \epsilon}.
  \end{equation}
  Applying \autoref{lem:magic}, we have $r_i \leq 1 + \epsilon$.
  In other words, $m_Q(\psi_i, \tau) \geq \frac{1}{1 + \epsilon} \cdot m^*_Q(\psi_i)$, which is what we intended to show.

  In summary, we define our reduction $(f, g, \alpha)$ as follows for all $x \in I$, $r > 1$, and $\tau \in S_Q(f(x, r))$:
  \begin{align*}
    f(x, r) & = \bigwedge_{i = 0}^{k - 1}{f_s(C(B, (x, i)))} \\
    g(x, \tau, r) & = T(\phi_j, g_s(\phi_j, \tau)) \\
    \alpha & = 2(r_P \log{r_P} + r_P - 1)\frac{1 + \epsilon}{\epsilon}
  \end{align*}
\end{proof}

\begin{theorem}[{\cite[Theorem~8.7]{book}}]\label{thm:minmax}
  For every minimization problem $P \in \APX$, where $P = (I, S, m_P, \min)$, there exists a maximization problem $Q \in \APX$ such that $P \APr Q$.
\end{theorem}
\begin{proof}
  Proof omitted; see \cite{book} for the proof.
\end{proof}

\begin{corollary}
  \textsc{Maximum 3-Satisfiability} is complete for \APX{} under polynomial time AP reductions.
\end{corollary}
\begin{proof}
  Follows from \autoref{thm:inncx}, \autoref{thm:maxcomplete}, and \autoref{thm:minmax}.
\end{proof}

\subsection{\texorpdfstring{\NCX}{NCX}-completeness}\label{ssc:ncxcomplete}

To show that \textsc{Maximum 3-Satisfiability} is complete for \NCX{} under \NC{} AP reductions we must show that it is in \NCX{} and that there is an \NC{} AP reduction from every optimization problem in \NCX{} to it.
By \autoref{thm:inncx}, we already know that it is in \NCX.
We will show that it is complete under \NC{} AP reductions for the class of maximization problems in \NCX.
Then we will show that there is an \NC{} AP reduction from every minimization problem to some maximization problem in \NCX.

We can reuse most of the auxiliary algorithms from the previous section, but we need a slight modification of \autoref{lem:interval}.

\begin{lemma}[\NNC{} version of \autoref{lem:interval}]\label{lem:ncinterval}
  Suppose $P \in \NCX$ and has measure function $m_P$, and let $A_P$ be the \FNC{} $r$-approximator for $P$, for some constant $r$.
  Let $r_n \geq 1$.
  There exists an \NNC{} circuit family, $\left\{B_\ell\right\}_{\ell \in \mathbb{N}}$, such that on input $(x, i)$, where $x \in I$ and $i$ is a natural number \emph{expressed in unary}, the circuit $B_\ell$, where $\ell = |x| + |i|$, decides if there exists a solution $y$ with
  \begin{equation*}
    {r_n}^i \cdot m_P(x, A_P(x)) \leq m_P(x, y) \leq {r_n}^{i + 1} \cdot m_P(x, A_P(x)).
  \end{equation*}
  Furthermore, if a solution $y$ exists, the output of $B_\ell$ is augmented with gates that output the bits of $y$.
\end{lemma}
\begin{proof}
  Let $p$ be the polynomial which bounds the size of the solutions for an instance $x$.
  Define $B$ as in \autoref{alg:ncinterval}.
  \begin{algorithm}
    \caption{\NNC{} algorithm that decides if there is an approximate solution for instance $x$ of problem $P$ in interval $i$; here $\ell = |x| + |i|$%
      \label{alg:ncinterval}}
    \begin{algorithmic}[1]
      \Require{$x \in I$, $i$ a natural number expressed in unary with $i \geq 0$}
      \Statex{}
      \Function{$B_\ell$}{$x$, $i$}:
        \State{\textbf{guess} $y$ with $|y| \leq p(|x|)$}
        \State{$a \gets m_P(x, A_P(x))$}
        \If{$(x, y) \in S$ and ${r_n}^i \cdot a \leq m_P(x, y) < {r_n}^{i + 1} \cdot a$}
          \State{\textbf{output} $y$}
          \State{\textbf{output} 1}
        \Else
          \State{\textbf{output} 0}
        \EndIf
      \EndFunction
    \end{algorithmic}
  \end{algorithm}
  By hypothesis, we know that $m_P$ and $A_P$ are in \FNC, and $S \in \NC$.
  Multiplication can be performed in \NC{} \cite{citationneeded}; let $M$ be the depth of that circuit, so $M \in O(\log{(n + i)})$.
  By repeated squaring, ${r_n}^i$ can be computed by an \FNC{} circuit whose depth is $O(M \log i)$, which is still polylogarithmic in $n + i$.
  Since inequalities can also be decided in \NC, the algorithm $B_\ell$ can be computed by an \NC{} circuit.
  Since $B_\ell$ outputs 1 if and only if there exists a solution $y$ with the stated bounds, the conclusion follows.
\end{proof}

\begin{theorem}[\NC{} version of \autoref{thm:maxcomplete}]\label{thm:ncmaxcomplete}
  \textsc{Maximum 3-Satisfiability} is complete for the class of maximization problems in \NCX{} under $\NCAPr$ reductions.
\end{theorem}
\begin{proof}
  The proof is nearly the same as the proof of \autoref{thm:maxcomplete}, replacing all polynomial time computable functions and procedures with those computable by \NC{} circuits.
  The main differences are as follows.

  When defining $f$, we use the \NC{} algorithm $B_\ell$ from \autoref{lem:ncinterval} instead of the polynomial time algorithm $B$ from \autoref{lem:interval}, so we have
  \begin{displaymath}
    f(x, r) = \bigwedge_{i = 0}^{k - 1}{f_s(C(B_\ell, (x, i)))},
  \end{displaymath}
  where $\ell = |x| + |i|$ and $i$ is expressed in unary.
  Observe that for each $i$, the formula $f_s(C(B_\ell, (x, i)))$ can be computed independently and in parallel.
  $B_\ell$ can be constructed in logarithmic space since it comes from a logarithmic space uniform family of circuits.
  Since $i < k$ and $k$ is a constant, $i$ can be written in unary in constant time when it is provided as input to the function $C$.
  $C$ is a logarithmic space computable function, and $f_s$ is in \NC.
  It follows that $f$ can be computed by an \NC{} circuit.

  When defining $g$, we again use $B_\ell$ instead of $B$, and express $i$ in unary where necessary.
  We again observe that for each $i$, the formula $\phi_i$ and the truth assignment $\tau_i$ can be computed independently and in parallel, as can deciding whether $\tau_i$ satisfies $\phi_i$.
  We can decide in logarithmic space whether a truth assignment satisfies a Boolean formula by \autoref{lem:evaluation}.
  Computing the maximum of a constant number of natural numbers, each of which has constant size, can be performed by an \NC{} circuit.
  It follows that $g$ can be computed by an \NC{} circuit.

  The value of $\alpha$ and the algebra proving the correctness of the reduction remains the same.
\end{proof}

\begin{theorem}[\NC{} version of \autoref{thm:minmax}]\label{thm:ncminmax}
  For every minimization problem $P \in \NCX$ there exists a maximization problem $Q \in \NCX$ such that $P \NCAPr Q$.
\end{theorem}
\begin{proof}
  A careful examination of the proof of \autoref{thm:minmax} reveals that if $P \in \NCX$, the necessary functions for the reduction are also computable in \NC.
  \begin{todo}
    Double check this.
  \end{todo}
\end{proof}

\begin{corollary}
  \textsc{Maximum 3-Satisfiability} is complete for \NCX{} under \NC{} AP reductions.
\end{corollary}
\begin{proof}
  Follows from \autoref{thm:inncx}, \autoref{thm:ncmaxcomplete}, and \autoref{thm:ncminmax}.
\end{proof}

\section*{About this work}

Copyright 2012 Jef{}frey Finkelstein.

This work is licensed under the Creative Commons Attribution-ShareAlike License 3.0.
Visit \mbox{\url{https://creativecommons.org/licenses/by-sa/3.0/}} to view a copy of this license.

The \LaTeX{} markup which generated this document is available on the World Wide Web at \mbox{\url{https://github.com/jfinkels/apxcompleteness}}.
It is also licensed under the Creative Commons Attribution-ShareAlike License.

The author can be contacted via email at \email{jeffreyf@bu.edu}.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
