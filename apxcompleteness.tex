\documentclass[]{article}

% Package `hyperref` must come before package `complexity`.
\usepackage[pdftitle={Revisiting the hardness of approximation of Maximum 3-Satisfiability}, pdfauthor={Jeffrey Finkelstein}]{hyperref}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{complexity}

\theoremstyle{plain}
%\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}{Corollary}
%\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{todo}{TODO}

\theoremstyle{definition}
\newtheorem{definition}{Definition}
%\newtheorem{openquestion}{Open question}

\mathchardef\mhyphen="2D

\newenvironment{instance}{\\Instance:}{}
\newenvironment{measure}{\\Measure:}{}
\newenvironment{proofidea}{\begin{proof}[Proof idea]}{\end{proof}}
\newenvironment{solution}{\\Solution:}{}
\newenvironment{question}{\\Question:}{}

\newcommand{\algorithmautorefname}{Algorithm}
\newcommand{\corollaryautorefname}{Corollary}
\newcommand{\definitionautorefname}{Definition}
\newcommand{\lemmaautorefname}{Lemma}

\algrenewcommand\algorithmicrequire{\textbf{Input:}}

\newcommand{\APr}{\leq_{AP}^{P}}
\newcommand{\ceil}[1]{\lceil{#1}\rceil}
\newcommand{\email}[1]{\href{mailto:#1}{\nolinkurl{#1}}}
%\newcommand{\lb}{\left\{}
\newcommand{\pair}[2]{{\left\langle{#1}, {#2}\right\rangle}}
%\newcommand{\pb}{\textsf{pb}}
%\newcommand{\rb}{\right\}}
%\newcommand{\st}{\,\middle|\,}

\author{Jef{}frey Finkelstein}
\date{\today}
\title{Revisiting the hardness of approximation of \texorpdfstring{\textsc{Maximum 3-Satisfiability}}{Maximum 3-Satisfiability}}

\begin{document}

\maketitle

\section{Introduction}

In this work, we attempt to clarify the existing proofs that \textsc{Maximum 3-Satisfiability} is both hard to approximate and complete for the class \APX{} under an appropriate type of reduction.
By doing this, we hope to provide not only a better understanding of the original proofs, but also a platform on which to base similar proofs.

\section{Preliminaries}

Throughout this work, $\Sigma=\{0, 1\}$, and $\Sigma^*$ is the set of all finite binary strings.
The length of a string $x \in \Sigma^*$ is denoted $|x|$.
We denote the set of positive rational numbers by $\mathbb{Q}^*$ and the natural numbers (excluding 0) by $\mathbb{N}$.
A decision problem is a subset of $\Sigma^*$.

\subsection{Probabilistically checkable proofs}

A \emph{$(\lg n, 1)$-verifier} is a probabilistic polynomial time Turing machine that, on input $(x, \pi, \rho)$ where $|x| = n$, $\pi$ is interpreted as a \emph{proof} (also known as a \emph{witness} or \emph{certificate}) and $\rho$ is interpreted as a sequence of random bits, reads at most $c \lg n$ random bits from $\rho$ and reads (with random access) at most $q$ bits from $\pi$, for some constants $c \in \mathbb{N}$ and $q \in \mathbb{N}$.
Define the complexity class $\PCP(\lg n, 1)$ (for \emph{probabilistically checkable proofs} with $O(\lg n)$ random bits and $O(1)$ proof queries) as follows: a decision problem $L \in \PCP(\lg n, 1)$ if there exists a $(\lg n, 1)$-verifier $V$ and a polynomial $p$ such that
\begin{enumerate}
\item
  if $x \in L$ then there exists a $\pi \in \Sigma^*$ with $|\pi| \leq p(|x|)$ such that
  \begin{displaymath}
    \Pr_{\rho \in \Sigma^{p(|x|)}}\left[V(x, \pi; \rho) \textnormal{ accepts}\right] = 1,
  \end{displaymath}
  and
\item
  if $x \notin L$ then for all $\pi \in \Sigma^*$ with $|\pi| \leq p(|x|)$, we have
  \begin{displaymath}
    \Pr_{\rho \in \Sigma^{p(|x|)}}\left[V(x, \pi; \rho) \textnormal{ accepts}\right] < \frac{1}{2}.
  \end{displaymath}
\end{enumerate}

\begin{theorem}[PCP Theorem \cite{pcp}]\label{thm:pcp}
  $\NP = \PCP(\lg n, 1)$.
\end{theorem}

\subsection{Optimization problems}

An optimization problem is given by $(I, S, m, t)$, where $I$ is called the \emph{instance set}, $S \subseteq I \times \Sigma^*$ and is called the \emph{solution relation}, $m \colon I \times \Sigma^* \to \mathbb{N}$ and is called the \emph{measure function}, and $t\in\{\max, \min\}$ and is called the \emph{type} of the optimization.
The set of solutions corresponding to an instance $x \in I$ is denoted $S_P(x)$.
The \emph{optimal measure} for any $x \in I$ is denoted $m^*(x)$.
Observe that for all $x \in I$ and all $y \in \Sigma^*$, if $t = \max$ then $m(x, y) \leq m^*(x)$ and if $t = \min$ then $m(x, y) \geq m^*(x)$.
The \emph{performance ratio}, $R$, of a solution $y$ for $x$ is
\begin{displaymath}
  R(x, y) = \max{\left(\frac{m^*(x)}{m(x, y)}, \frac{m(x, y)}{m^*(x, y)}\right)}.
\end{displaymath}
A function $f \colon I \to \Sigma^*$ is an \emph{$r(n)$-approximator} if $R(x, f(x)) \leq r(|x|)$, for some function $r \colon \mathbb{N} \to \mathbb{Q}^*$.
If $r(n)$ is a constant function with value $\delta$, we simply say $f$ is a \emph{$\delta$-approximator}.

For the sake of simplicity, in \autoref{ssc:gaps} and \autoref{sec:hardness} (the sections dealing with hardness of approximation) we will consider \emph{only} maximization problems, although the definitions and results also hold for minimization problems, with the appropriate (slight) changes to the definitions (for example, in \autoref{def:intro} we would require that $m^*(f(x)) > (1 + \epsilon) \cdot c(x)$ instead of $m^*(f(x)) < \frac{1}{1 + \epsilon} \cdot c(x)$).

\subsubsection{Classes of optimization problems}

The complexity class \NPO{} is the class of all optimization problems $(I, S, m, t)$ such that the following conditions hold.
\begin{enumerate}
\item The instance set $I$ is decidable by a deterministic polynomial time Turing machine.
\item The solution relation $S$ is decidable by a deterministic polynomial time Turing machine and is polynomially bounded (that is, the length of $y$ is bounded by a polynomial in the length of $x$ for all $(x, y)\in S$).
\item The measure function $m$ is computable by a deterministic polynomial time Turing machine.
\end{enumerate}

The complexity class \APX{} is the subclass of \NPO{} in which for each optimization problem $P$ there exists a polynomial time computable $r$-approximator for $P$, where $r(n) \in O(1)$ for all $n \in \mathbb{N}$.

\subsubsection{Reductions among optimization problems}

\begin{definition}[{\cite[Definition~8.3]{book}}]
  Let $P$ and $Q$ be optimization problems, so $P = (I_P, S_P, m_P, t_P)$ and $Q = (I_Q, S_Q, m_Q, t_Q)$.
  There is an \emph{AP reduction from $P$ to $Q$}, denoted $P \APr Q$, if there exist functions $f \colon I_P \times \mathbb{Q}^+ \to I_Q$ and $g \colon I_P \times \Sigma^* \times \mathbb{Q}^+ \to \Sigma^*$ and constant $\alpha \in \mathbb{Q}^+$ with $\alpha \geq 1$ such that:
  \begin{enumerate}
  \item For all $x \in I_P$ and all $r \in \mathbb{Q}^+$ with $r > 1$, we have $f(x, r) \in I_P$.
  \item For all $x \in I_P$ and all $r \in \mathbb{Q}^+$ with $r > 1$, if $S_P(x) \neq \emptyset$ then $S_Q(f(x, r)) \neq \emptyset$.
  \item For all $x \in I_P$, all $r \in \mathbb{Q}^+$ with $r > 1$, and all $y \in S_Q(f(x, r))$, we have $g(x, y, r) \in S_P(x)$.
  \item $f$ is computable in polynomial time with respect to $x$ and $g$ is computable in polynomial time with respect to $x$ and $y$.
  \item For all $x \in I_P$, all $r \in \mathbb{Q}^+$ with $r > 1$, and all $y \in S_Q(f(x, r))$, we have
    \begin{equation*}
      R_Q(f(x, r), y) \leq r \text{ implies } R_P(x, g(x, y, r)) \leq 1 + \alpha \cdot (r - 1).
    \end{equation*}
  \end{enumerate}
  We identify such a reduction with the triple $(f, g, \alpha)$.
\end{definition}

Although the definition of AP reduction allows $f$ and $g$ access to $r$, the quality of the approximation, in most known natural reductions, this access is not necessary.
Therefore, in some cases, we will write $f(x)$ and $g(x, y)$ instead of $f(x, r)$ and $g(x, y, r)$.

There are a wealth of other kinds of reductions among optimization problems designed to capture the notion that a good approximate solution for problem $Q$ implies a good approximate solution for problem $P$.
Some experts suggest that the AP reduction is the correct reduction to use to show, for example, completeness in \APX.
For a survey of other approximation preserving reductions, see \cite{crescenzi97}.

\subsection{Boolean formulae}

A \emph{literal} is a variable or its negation.
A Boolean formula is in \emph{conjunctive normal form (CNF)} if it is a conjunction of clauses, each of which is a disjunction of literals.
A Boolean formula is called \emph{$k$-CNF} if it is in conjunctive normal form and each of the clauses has at most $k$ literals.
The set of variables which appear in a Boolean formula is $Var(\phi)$.
A \emph{truth assignment} to a Boolean formula $\phi$ is a function $\tau \colon Var(\phi) \to \{0, 1\}$.
A truth assignment \emph{satisfies} a Boolean formula $\phi$ if replacing the variables in $\phi$ with their corresponding truth assignments yields a true formula.

\begin{definition}[\textsc{Satisfiability}]
  \mbox{}
  \begin{instance}
    a Boolean formula $\phi$
  \end{instance}
  \begin{question}
    Is there a truth assignment $\tau$ to $\phi$ such that $\tau$ satisfies $\phi$?
  \end{question}
\end{definition}

\begin{definition}[\textsc{3-Satisfiability}]
  \mbox{}
  \begin{instance}
    a 3-CNF Boolean formula $\phi$
  \end{instance}
  \begin{question}
    Is there a truth assignment $\tau$ to $\phi$ such that $\tau$ satisfies $\phi$?
  \end{question}
\end{definition}

\begin{definition}[\textsc{Maximum 3-Satisfiability}]
  \mbox{}
  \begin{instance}
    a 3-CNF Boolean formula $\phi$
  \end{instance}
  \begin{solution}
    a truth assignment $\tau$ to $\phi$
  \end{solution}
  \begin{measure}
    the number of satisfied clauses in $\phi$
  \end{measure}
\end{definition}

\begin{lemma}\label{lem:half}
 At least half of the clauses of any Boolean formula in conjunctive normal form are always satisfiable.
\end{lemma}

\begin{lemma}[{\cite[Example~6.5]{book}}]\label{lem:three}
  There is a polynomial time computable function $T$ that maps a Boolean formula in conjunctive normal form with $c$ clauses and at most $k$ literals per clause into an equivalent Boolean formula in conjunctive normal form with $c \cdot (k - 2)$ clauses and at most three literals per clause.
\end{lemma}

\subsection{Gap-introducing and gap-preserving reductions}\label{ssc:gaps}

\begin{definition}[{\cite[Section~29.1]{vazirani}}]\label{def:intro}
  Let $P$ be a decision problem and $Q$ be maximization problem with $Q = (I, S, m, \max)$.
  There is a \emph{gap-introducing reduction} from $P$ to $Q$ if there exist polynomial time computable functions $f \colon \Sigma^* \to I$ and $c \colon \Sigma^* \to \mathbb{N}$ and an $\epsilon \in \mathbb{Q}^+$ such that for any $x \in \Sigma^*$,
  \begin{enumerate}
  \item if $x \in P$ then $m^*(f(x)) \geq c(x)$, and
  \item if $x \notin P$ then $m^*(f(x)) < \frac{1}{1 + \epsilon} \cdot c(x)$.
  \end{enumerate}
  We call $\epsilon$ the \emph{gap parameter} of the reduction.
\end{definition}

Notice that as the value of $\epsilon$ increases, so does the ``gap'' between the optimal measures corresponding to strings in $P$ and strings not in $P$.

\begin{definition}
  Let $P\in\NP$ and $Q$ be a maximization problem in \NPO, with $Q = (I, S, m, \max)$.
  Let $R_P$ be the \NP-relation induced by $P$.
  Suppose there is a gap-introducing reduction $(f, c, \epsilon)$ from $P$ to $Q$.
  The gap-introducing reduction is \emph{enhanced} if there exists a polynomial time computable function $g \colon I \times \Sigma^* \to \Sigma^*$ such that for all $x \in I$ and all $y \in S(f(x))$ with $m(f(x), y) \geq \frac{1}{1 + \epsilon} \cdot c(x)$, we have $\left(x, g(x, y)\right) \in R_P$ if and only if $x \in P$.
\end{definition}

An enhanced gap-introducing reduction is a gap-introducing reduction by definition.
Notice that from \autoref{lem:xinp} it is vacuously true that $\left(x, g(x, y)\right) \in R_P$ implies $x \in P$, since the conclusion is always true.
The interesting case is the converse, because it requires that $g$ produce a witness that $x$ is in $P$ even though it may only have an approximate solution $y$.

\begin{lemma}\label{lem:xinp}
  If there is a gap-introducing reduction $(f, c, \epsilon)$ from $P$ to $Q$, where $P = (I_P, S_P, m_P, \max)$ and $Q = (I_Q, S_Q, m_Q, \max)$, then for all $x \in I_P$ and all $y \in S_Q(f(x))$, if $m(f(x), y) \geq \frac{1}{1 + \epsilon} \cdot c(x)$ then $x \in P$.
\end{lemma}
\begin{proof}
  Since $m^*(f(x)) \geq m(f(x), y) \geq \frac{1}{1 + \epsilon} \cdot c(x)$, we have $x \in P$ by condition 2 of the gap-introducing reduction.
\end{proof}

\begin{definition}[{\cite[Section~29.1]{vazirani}}]
  Let $P$ and $Q$ be maximization problems, where $P = (I_P, S_P, m_P, \max)$ and $Q = (I_Q, S_Q, m_Q, \max)$.
  There is a \emph{gap-preserving reduction} from $P$ to $Q$ if there exist a function $f \colon I_P \to I_Q$, functions $c_P, c_Q \colon \mathbb{N} \to \mathbb{N}$, and functions $\alpha, \beta \colon \mathbb{N} \to \mathbb{Q}^*$, with $\alpha(n) \geq 0$ and $\beta(n) \geq 0$ for all $n \in \mathbb{N}$, all polynomial time computable, such that
  \begin{itemize}
  \item if $m^*_P(x) \geq c_P(x)$ then $m^*_Q(f(x)) \geq c_Q(f(x))$, and
  \item if $m^*_P(x) < \frac{1}{1 + \alpha(|x|)} \cdot c_P(x)$ then $m^*_Q(f(x)) < \frac{1}{1 + \beta(|f(x)|)} \cdot c_Q(f(x))$.
  \end{itemize}
  We identify such a reduction with the five-tuple $(f, c_P, c_Q, \alpha, \beta)$.
\end{definition}

Intuitively, the gap in $P$ scales with $\alpha$ and the gap in $Q$ with $\beta$.
Like the enhanced gap-introducing reduction, there is also an enhanced gap-preserving reduction.

\begin{definition}
  Let $P$ and $Q$ be two maximization problems, defined by $P = (I_P, S_P, m_P, \max)$ and $Q = (I_Q, S_Q, m_Q, \max)$.
  Suppose that there exists a gap-preserving reduction given by $(f, c_P, c_Q, \alpha, \beta)$ from $P$ to $Q$.
  The gap-preserving reduction is \emph{enhanced} if there exists a polynomial time computable function $g \colon I_P \times \Sigma^* \to \Sigma^*$ such that for all $x \in I_P$ and all $y \in S_Q(f(x))$, we have $g'(x, y) \in S_P(x)$ and $m_P(x, g'(x, y)) \geq \frac{1}{1 + \alpha(|x|)} \cdot c_P(x)$.
  We identify such a reduction with the six-tuple $(f, g, c_P, c_Q, \alpha, \beta)$.
\end{definition}

\begin{lemma}\label{lem:compose}
  Let $P$ be a decision problem and $Q$ and $T$ be maximization problems.
  Suppose there is a polynomial time enhanced gap-introducing reduction $(f, g, c, \epsilon)$ from $P$ to $Q$ and a polynomial time enhanced gap-preserving reduction $(f', g', c_Q, c_T, \alpha, \beta)$ from $Q$ to $T$.
  If $c(x) \geq c_Q(f(x))$ and $\frac{1}{1 + \epsilon} \cdot c(x) \leq \frac{1}{1 + \alpha(|f(x)|)} \cdot c_Q(f(x))$ then $(\tilde{f}, \tilde{g}, \tilde{c}, \tilde{\epsilon})$ is an enhanced gap-introducing reduction from $P$ to $Q$, where
  \begin{align*}
    \tilde{f}(x) & = f'(f(x)), \\
    \tilde{g}(x, y) & = g(x, g'(f(x), y)), \\
    \tilde{c}(x) & = c_T(f'(f(x))), \text{ and} \\
    \tilde{\epsilon} & = \beta(|f'(f(x))|).
  \end{align*}
\end{lemma}
\begin{proof}
  Suppose first that $x \in P$.
  Our goal is to show that $m^*_T(f'(f(x))) \geq \tilde{c}(x)$.
  Since we have a gap-introducing reduction from $P$ to $Q$, we know $m^*_Q(x) \geq c(x)$.
  Since we have a gap-preserving reduction from $Q$ to $T$, we know that if $m^*_Q(f(x)) \geq c_Q(f(x))$ then $m^*_T(f'(f(x))) \geq c_T(f'(f(x)))$.
  Since $c(x) \geq c_Q(f(x))$ by hypothesis, it follows that $m^*_T(f'(f(x))) \geq c_T(f'(f(x))) = \tilde{c}(x)$.

  Suppose now that $x \notin P$.
  Our goal is to show that $M^*_T(f'(f(x))) < \frac{1}{1 + \tilde{\epsilon}} \cdot \tilde{c}(x)$.
  Since we have a gap-introducing reduction from $P$ to $Q$, we know $m^*_Q(f(x)) < \frac{1}{1 + \epsilon} \cdot c(x)$.
  Since we have a gap-preserving reduction from $Q$ to $T$, we know that if $m^*_Q(f(x)) < \frac{1}{1 + \alpha(|f(x)|)} \cdot c_Q(f(x))$ then $m^*_T(f'(f(x))) < \frac{1}{1 + \beta(|f'(f(x))|)} \cdot c_T(f'(f(x)))$.
  Since $\frac{1}{1 + \epsilon} \cdot c(x) \leq \frac{1}{1 + \alpha(|f(x)|)} \cdot c_Q(f(x))$ by hypothesis, it follows that $m^*_T(f'(f(x))) < \frac{1}{1 + \beta(|f'(f(x))|)} \cdot c_T(f'(f(x))) = \frac{1}{1 + \tilde{\epsilon}} \cdot \tilde{c}(x)$.

  We have now shown that the stated reduction is gap-introducing; it remains to show that it is also enhanced.
  Suppose $y_T \in S_T(f'(f(x)))$ and $m_T(f'(f(x)), y_T) \geq \frac{1}{1 + \tilde{\epsilon}} \cdot \tilde{c}(x)$.
  Our goal is to show that $(x, \tilde{g}(x, y_T)) \in R_P$ if and only if $x \in P$.
  Since we have an enhanced gap-preserving reduction from $Q$ to $T$, we know that $g'(f(x), y_T) \in S_Q(f(x))$ and $m_Q(f(x), g'(f(x), y_T)) \geq \frac{1}{1 + \alpha(|f(x)|)} \cdot c_Q(f(x))$.
  Since we have an enhanced gap-introducing reduction from $P$ to $Q$ and $\frac{1}{1 + \alpha(|f(x)|)} \cdot c_Q(f(x)) \geq \frac{1}{1 + \epsilon} \cdot c(x)$ by hypothesis, we know that $(x, g(x, g'(f(x), y_T))) \in R_P$ if and only if $x \in P$.
  Hence $(x, \tilde{g}(x, y_T)) \in R_P$ if and only if $(x, g(x, g'(f(x), y_T))) \in R_P$ if and only if $x \in P$.

  Since we have proven the three conditions required for an enhanced gap-introducing reduction, we finally conclude that $(\tilde{f}, \tilde{g}, \tilde{c}, \tilde{\epsilon})$ is an enhanced gap-introducing reduction from $P$ to $T$.
\end{proof}

\section{Hardness of approximation}\label{sec:hardness}

In this section we use the PCP theorem to prove \textsc{Maximum 3-Satisfiability} is hard to approximate (specifically, that for certain constant factors, no approximation can exist unless $\P = \NP$).
We do this by reducing \textsc{Satisfiability} to \textsc{Maximum $k$-Function Satisfiability}, and then reducing that to \textsc{Maximum 3-Satisfiability}, ensuring that the reductions compose.

The main tool of this section is the following theorem, which provides sufficient conditions for an optimization problem in \NPO{} to be hard to approximate.

\begin{theorem}[{\cite[Theorem~3.7]{book}}]\label{thm:gap}
  Let $P$ be an \NP-complete decision problem and $Q$ be a maximization problem in \NPO.
  If there is a polynomial time gap-introducing reduction from $P$ to $Q$ with gap parameter $\epsilon$, then for all $r < 1 + \epsilon$, there is no polynomial time $r$-approximator for $P$ unless $\P = \NP$.
\end{theorem}
\begin{proof}
  In order to produce a contradiction, suppose that there exists a polynomial time $r$-approximator $A$ for $Q$ with $r < 1 + \epsilon$.
  Let $f$ and $c$ be the functions which define the gap-introducing reduction.
  Define algorithm $A'$ as follows on input $x$: $A'(x)$ accepts if and only if $m(f(x), A(f(x))) > \frac{1}{1 + \epsilon} \cdot c(x)$, for all $x\in\Sigma^*$.
  We will show that $A'$ is a polynomial time algorithm for the \NP-complete problem $P$, and hence $\P=\NP$.

  Since $f$, $A$, $c$, and basic arithmetic operations such as addition and multiplication are polynomial time computable functions, so is $A'$.
  In order to show that $A'$ is correct we must consider two cases.
  \begin{enumerate}
  \item
    Suppose $x \in P$.
    Since $A$ is an $r$-approximator for a maximization problem and $r < 1 + \epsilon$, we have
    \begin{displaymath}
      \frac{m^*(f(x))}{m(f(x), A(f(x)))} \leq r < 1 + \epsilon,
    \end{displaymath}
    which implies
    \begin{displaymath}
      \frac{m(f(x), A(f(x)))}{m^*(f(x))} \geq \frac{1}{r} > \frac{1}{1 + \epsilon}.
    \end{displaymath}
    Since $m^*(f(x)) \geq c(x)$ by hypothesis, this implies
    \begin{align*}
      m(f(x), A(f(x))) & > \frac{1}{1 + \epsilon} \cdot m^*(f(x)) \\
      & \geq \frac{1}{1 + \epsilon} \cdot c(x).
    \end{align*}
    Hence $A'$ will accept on input $x$.
  \item
    If $x \notin P$ then $m^*(f(x)) < \frac{1}{1 + \epsilon} \cdot c(x)$ by hypothesis.
    Since $Q$ is a maximization problem, $m(f(x), A(f(x))) \leq m^*(f(x))$.
    Combining the two inequalities, we find $m(f(x), A(f(x))) < \frac{1}{1 + \epsilon} \cdot c(x)$.
    Hence, $A'$ will reject on input $x$.
  \end{enumerate}
  We have shown that $A'$ is a correct polynomial-time computable algorithm which decides the \NP-complete problem $P$, and the conclusion follows.
\end{proof}

\begin{corollary}\label{cor:notinptas}
  Let $P$ be an \NP-complete decision problem and $Q$ be a maximization problem in \NPO.
  If there is a polynomial time gap-introducing reduction from $P$ to $Q$ with gap parameter $\epsilon$ then $Q \notin \PTAS$ unless $\P = \NP$.
\end{corollary}
\begin{proof}
  In order to produce a contradiction, suppose that there exists a polynomial time approximation scheme for $Q$, so there exists a function $f$ such that on input $x$ and $N$, $f$ produces solutions for $Q$, $f$ is computable in time polynomial in the length of $x$, and $R(x, f(x, N)) \leq 1 + \frac{1}{N}$ for all $x \in \Sigma^*$ and all $N \in \mathbb{N}$.
  Specifically, if we choose $N > \frac{1}{\epsilon}$ then $R(x, f(x, N)) \leq 1 + \frac{1}{N} < 1 + \epsilon$, for all $x \in \Sigma^*$.
  Therefore, $f$ is in fact a polynomial time $r$-approximator for $Q$, where $r < 1 + \epsilon$.
  The conclusion follows from \autoref{thm:hard}.
\end{proof}

In order to show that \textsc{Maximum 3-Satisfiability} is hard to approximate, we will show a reduction to it from \textsc{Satisfiability} using an intermediate maximization problem, \textsc{Maximum $k$-Function Satisfiability}.

\begin{definition}[\textsc{$k$-Function Satisfiability}]
  \mbox{}
  \begin{instance}
    finite set of Boolean variables $x_1, x_2, \ldots, x_n$ and a finite set of  functions $g_1, g_2, \ldots, g_m$, each of which is a function of $k$ of the variables, where $k \geq 2$
  \end{instance}
  \begin{question}
    Does there exist a truth assignment $\tau$ such that all the functions are satisfied?
  \end{question}
\end{definition}

\begin{definition}[\textsc{Maximum $k$-Function Satisfiability}]
  \mbox{}
  \begin{instance}
    finite set of Boolean variables $x_1, x_2, \ldots, x_n$ and a finite set of  functions $g_1, g_2, \ldots, g_m$, each of which is a function of $k$ of the variables, where $k \geq 2$
  \end{instance}
  \begin{solution}
    a truth assignment $\tau$ to the variables
  \end{solution}
  \begin{measure}
   the number of satisfied functions
  \end{measure}
\end{definition}

Observe that $\textsc{Maximum \textit{k}-Satisfiability} \in \NPO$.

\begin{lemma}[{\cite[Lemma~29.10]{vazirani}}]\label{lem:intro}
  There is an enhanced gap-introducing reduction from \textsc{Satisfiability} to \textsc{Maximum $k$-Function Satisfiability} with gap parameter $1$, for some $k \in \mathbb{N}$.
\end{lemma}
\begin{proof}
  For the sake of brevity, we will let $P = \textsc{Satisfiability}$ and $Q = \textsc{Maximum \textit{k}-Function Satisfiability}$.

  By \autoref{thm:pcp}, there exists a $(\lg n, 1)$-verifier for $P$.
  Let $\phi$ be a Boolean formula of length $n$, which will be the input to the verifier.
  Let $V$ be that verifier, let $c$ and $q$ be the constants such that $V$ uses at most $c \lg n$ random bits and at most $q$ bits of the proof.
  For simplicity, let the length of the random string input to $V$ be exactly $c \lg n$.
  When considering all possible random strings $\rho$ from which $V$ reads its random bits, $V$ reads a total of at most $q \cdot 2^{c \lg n}$ bits of the proof, which equals $q \cdot n^c$.
  Let $B$ be the set of at most $q \cdot n^c$ Boolean variables representing the values of the bits of the proof at the queried locations.

  We let $k = q$, and then for each string $\rho$ (of length $c \lg n$), we will define a function $g_\rho$ which is a function of at most $q$ of the Boolean variables from $B$.
  Consider the Cook-Levin transformation of the operation of the verifier $V$ on input $(\phi, \tau; \rho)$, where $\phi$ is a Boolean formula (an instance of $P$) and $\tau$ is a satisfying assignment to the variables of $\phi$.
  Let $\psi$ be the Boolean formula over the variables $z_1, z_2, \ldots, z_{h(n)}$ produced by this transformation, where $h(n)$ is the polynomial bounding the size of the Boolean formula produced by the Cook-Levin transformation.
  Some of these variables depend on the bits of $\phi$, some depend on $q$ bits of $\tau$, and some depend on the bits of $\rho$ (and these sets may intersect).
  If we let $\phi$ and $\rho$ be fixed, however, we can define $g_\rho$ to be the restriction of the Boolean function $\psi$ to the $q$ variables of $\psi$ corresponding to the $q$ bits of $\tau$ read by the verifier on input $(\phi, \tau; \rho)$.

  Let $G = \left\{g_\rho \,\middle|\, \rho \in \Sigma^* \textnormal{ and } |\rho| = c \lg n \right\}$.
  Notice that $|G| = 2^{c \lg n} = n^c$.
  Now we define the enhanced gap-introducing reduction as follows for all Boolean formulae $\phi$ of length $n$, and all truth assignments $\tau$ to the variables in $B$ (\emph{not} the variables of $\phi$):
  \begin{align*}
    f(\phi) & = (B, G) \\
    c(\phi) & = n^c \\
    \epsilon & = 1 \\
    g(\phi, \tau) & = \tau^+
  \end{align*}
  where $\tau^+$ is the extension of $\tau$ in which any variables in $\phi$ not in $B$ are assigned an arbitrary binary value (say 0).
  The function $c$ is polynomial time computable because exponentiation of $n$ to a constant power is polynomial time computable.
  The function $f$ is polynomial time computable because both the size of $B$ and the size of $G$ are polynomial in $n$, and the length of each of their elements is polynomial in $n$ as well.
  The function $g$ is polynomial time computable because it simply copies $\tau$ to its output along with a polynomial number of extra bits.

  If $\phi \in P$ then there is a truth assignment $\tau$ such that $V$ accepts on input $(\phi, \tau; \rho)$ with probability 1 over the random strings $\rho$.
  In this case, $m^*(f(\phi)) = m^*((B, G)) = n^c$, since all the functions $g_\rho$ in $G$ are satisfied.
  If $\phi \notin P$ then for every truth assignment, the verifier accepts with probability at most $\frac{1}{2}$.
  In this case, every truth assignment satisfies less than half of all the $n^c$ functions in $G$.
  Hence $m^*(f(\phi)) = m^*((B, G)) < \frac{1}{2} \cdot n^c = \frac{1}{1 + \epsilon} c(\phi)$.
  
  To show that this gap-introducing reduction is enhanced, we suppose $\tau$ is a satisfying assignment for $f(\phi)$ which satisfies at least $\frac{1}{2} \cdot n^c$ of the functions in $f(\phi)$.
  Let $R_P$ be the \NP-relation corresponding to $P$.
  Our goal is to show that $(\phi, g(\phi, \tau)) \in R_P$ if and only if $\phi \in P$.
  By \autoref{lem:xinp}, it suffices to show that $\phi \in P$ implies $(\phi, g(\phi, \tau)) \in R_P$.
  If $\phi$ is satisfiable, then all its clauses are satisfiable.
  Subsequently, all the functions in $f(\phi)$ are satisfiable.
  Then $g(\phi, \tau)$, which equals $\tau^+$, is a truth assignment on which the verifier $V$ will accept with probability 1 (it ignores the bits of $\tau^+$ not in $\tau$).
  Hence $(\phi, g(\phi, \tau)) \in R_P$.

  Therefore we have constructed a polynomial time enhanced gap-introducing reduction with gap parameter 1 from $P$ to $Q$ (that is, from \textsc{Satisfiability} to \textsc{Maximum $k$-Function Satisfiability}).
\end{proof}

\begin{lemma}[{\cite[Proof of Theorem~29.7]{vazirani}}]\label{lem:decision}
  There is a polynomial time many-one reduction from \textsc{$k$-Function Satisfiability} to \textsc{3-Satisfiability}.
\end{lemma}
\begin{proof}
  Let $(\{x_1, x_2, \ldots, x_n\}, \{f_1, f_2, \ldots, f_m\})$ be an instance of \textsc{$k$-Function Satisfiability}.
  Without loss of generality, each function $f_i$ of $k$ variables can be written as a Boolean formula containing at most $2^k$ clauses, in which each clause contains at most $k$ literals (why?).
  Call this Boolean formula $\psi_i$, and let $\psi = \bigwedge_{i = 1}^m{\psi_i}$.
  Observe that the given instance of \textsc{$k$-Function Satisfiability} is satisfiable if and only if $\psi$ is satisfiable.

  Let $T$ be the polynomial time computable function from \autoref{lem:three}.
  Now we can define the reduction $g$ by $g(\phi) = T(\psi)$ for all Boolean formulae $\phi$.
  The total number of clauses in $\psi$ is at most $m \cdot 2 ^ k$, so the total number of clauses in $T(\psi)$ is at most $m \cdot 2^k \cdot (k - 2)$ (still a polynomial in the length of the input since $k$ is considered a fixed constant).
  It is clear that $g$ is polynomial time computable, and $\phi$ is satisfiable if and only if $\psi$ is satisfiable if and only if $T(\psi)$ is satisfiable.
\end{proof}

\begin{lemma}\label{lem:opt}
  There exists a polynomial time enhanced gap-preserving reduction from \textsc{Maximum $k$-Function Satisfiability} to \textsc{Maximum 3-Satisfiability}.
  %, with preservation factor $\alpha$ defined by $\alpha = 2^{k + 1} - 1$.
  % which is greater than $\alpha = 2^{k + 1} \cdot (k - 2) - 1$.
\end{lemma}
\begin{proof}
  For brevity, we let $P = \textsc{Maximum \textit{k}-Function Satisfiability}$ and $Q = \textsc{Maximum 3-Satisfiability}$.

  We assume $k$ is a fixed constant.
  Let $f_0$ be the reduction given in \autoref{lem:decision} from \textsc{$k$-Function Satisfiability} to \textsc{3-Satisfiability}.
  Define the six functions as follows for all finite sets of Boolean variables $U$, all finite sets of Boolean functions $F$ of at most $k$ of the variables in $U$, and all truth assignments $\tau$ to 3-CNF Boolean formulae:
  \begin{align*}
    f(\pair{U}{F}) & = f_0(\pair{U}{F}) \\
    g(\pair{U}{F}, \tau) & = \tau|_U \\
    c_P(\pair{U}{F}) & = |F| \\
    c_Q(\phi) & = |\phi| \\
    \alpha(n + M) & = 1 \\
    \beta(n + M) & = ???
  \end{align*}
  where $\tau|_U$ represents the restriction of the satisfying truth assignment $\tau$ to only the variables of $U$.
  Intuitively, the definitions of $c_P$ and $c_Q$ suggest that if all functions in $F$ are satisfiable then all clauses in $f(\pair{U}{F})$ will be satisfiable (there are at most $M \cdot 2^k \cdot (k - 2)$ of them, from the proof of \autoref{lem:decision}), and the definitions of $\alpha$ and $\beta$ suggest ...
  \begin{todo}
    The number of clauses in $f(\pair{U}{F})$ is \emph{at most} $M \cdot 2^k \cdot (k - 2)$.
    Isn't this a problem?
  \end{todo}

  Suppose $m^*_P(\pair{U}{F}) \geq c_P(x) = |F|$.
  We know that $m^*_Q(f(\pair{U}{F})) = m^*_Q(f_0(\pair{U}{F})) = |F| \cdot 2^k \cdot (k - 2) = c_Q(f_0(\pair{U}{F}))$.
  \begin{todo}
    Same problem as above; requires $f_0(\pair{U}{F})$ to have exactly $M \cdot 2^k \cdot (k - 2)$ clauses.
  \end{todo}

  Suppose $m^*_P(\pair{U}{F}) < \frac{1}{2} \cdot M$.
  \begin{todo}
    The number of clauses satisfiable in the transformed 3-SAT instance may not have anything to do with the number of clauses satisfiable in the original functions.
  \end{todo}
\end{proof}

\begin{lemma}\label{lem:gap3}
  There is an enhanced gap-introducing reduction from \textsc{Satisfiability} to \textsc{Maximum 3-Satisfiability}.
  %% with gap parameter $\epsilon$ defined by
  %% \begin{displaymath}
  %%   \epsilon = \frac{1}{2^{k + 1} - 1},
  %% \end{displaymath}
  %% where $k$ is a constant coming from the intermediate reduction from \textsc{Maximum $k$-Function Satisfiability} to \textsc{Maximum 3-Satisfiability}.
\end{lemma}
\begin{proof}
  We will use \autoref{lem:intro} and \autoref{lem:opt} to invoke \autoref{lem:compose}.

  We will let $(f, g, c, \epsilon)$ be the enhanced gap-introducing reduction and let $(f', g', c_Q, c_T, \alpha, \beta)$ be the enhanced gap-preserving reduction.
  The enhanced gap-introducing reduction of \autoref{lem:intro} has
  \begin{align*}
    c(\phi) & = n^{c_0}, \text{ and} \\
    \epsilon & = 1,
  \end{align*}
  where $n = |\phi|$ and $c_0$ is a constant which comes from the proof of \autoref{lem:intro} (specifically, from bounding the number of random bits read by the verifier).
  The enhanced gap-preserving reduction of \autoref{lem:opt} has
  \begin{align*}
    c_Q(\pair{U}{F}) & = M, \\
    c_T(\phi) & = M \cdot 2^k \cdot (k - 2), \\
    \alpha(n + M) & = 1, \text{ and} \\
    \beta(n + M) & = ???.
  \end{align*}

  To meet the conditions in the hypothesis of \autoref{lem:compose}, we need $c(\phi) \geq c_Q(f(\phi))$ and $\frac{1}{1 + \epsilon} \cdot c(\phi) \leq \frac{1}{1 + \alpha(|f(\phi)|)} \cdot c_Q(f(\phi))$.
  Since $c(\phi) = n^{c_0}$ and $c_Q(f(\phi)) = c_Q(\pair{B}{G}) = n^{c_0} \cdot 2^k \cdot (k - 2)$, the first inequality is satisfied (for $k \geq 3$).
  Since $\frac{1}{1 + \epsilon} \cdot c(\phi) = \frac{1}{2} \cdot n^{c_0}$ and $\frac{1}{1 + \alpha(|f(\phi)|)} \cdot c_Q(f(\phi)) = \frac{1}{2} \cdot c_q(\pair{B}{G}) = \frac{1}{2} \cdot n^{c_0} \cdot 2^k \cdot (k - 2)$, the second inequality is satisfied (again for $k \geq 3$).

  Therefore we can conclude that there is an enhanced gap-introducing reduction $(\tilde{f}, \tilde{g}, \tilde{c}, \tilde{\epsilon})$ from \textsc{Satisfiability} to \textsc{Maximum 3-Satisfiability}, where
  \begin{align*}
    \tilde{f}(\phi) & = f'(f(\phi)) \\
    & = f'(\pair{B}{G}) \\
    & = f_0(\pair{B}{G}), \\
    \tilde{g}(\phi, \tau) & = g(\phi, g'(f(\phi), \tau)) \\
    & = g(\phi, g'(\pair{B}{G}, \tau)) \\
    & = g(\phi, \tau|_B) \\
    & = {\tau|_B}^+, \\
    \tilde{c}(\phi) & = c_T(f'(f(x))) \\
    & = c_T(f'(\pair{B}{G})) \\
    & = c_T(f_0(\pair{B}{G})) \\
    & = |f_0(\pair{B}{G})| \\
    & = |G| \cdot 2^k \cdot (k - 2), \text{ and} \\
    \tilde{\epsilon} & = \beta(|f'(f(x))|) \\
    & = ???. \qedhere
  \end{align*}
\end{proof}

Although \autoref{lem:gap3} specifies an enhanced gap-introducing reduction, the following theorem does not require that the reduction be enhanced.
However, we provide the enhanced reduction because \autoref{lem:gap3} will be used in \autoref{sec:complete}.

\begin{theorem}[{\cite[Theorem~6.3]{book}} and {\cite[Corollary~29.8]{vazirani}}]\label{thm:hard}
  There is a constant $k$ such that no polynomial time $r$-approximator for \textsc{Maximum 3-Satisfiability} exists unless $\P = \NP$, for all $r < 1 + ???$.
\end{theorem}
\begin{proof}
  Follows from \autoref{lem:gap3} and \autoref{thm:gap}.
\end{proof}

\begin{corollary}
  $\textsc{Maximum 3-Satisfiability} \notin \PTAS$ unless $\P = \NP$.
\end{corollary}
\begin{proof}
  Follows from \autoref{thm:hard} and \autoref{cor:notinptas}.
\end{proof}

\section{\texorpdfstring{\APX}{APX}-completeness}\label{sec:complete}

To show that \textsc{Maximum 3-Satisfiability} is complete for \APX{} under AP reductions we must show that it is in \APX{} and that there is an AP reduction from every optimization problem in \APX{} to it.
We will do this in three steps.
First, we will show that it is in \APX.
Second, we will show that it is complete under AP reductions for the class of maximization problems in \APX.
Finally, we will show that there is an AP reduction from every minimization problem to some maximization problem in \APX.

\begin{theorem}\label{thm:inapx}
  $\textsc{Maximum 3-Satisfiability} \in \APX$.
\end{theorem}
\begin{proof}
  \textsc{Maximum 3-Satisfiability} is a restriction of \textsc{Satisfiability}, and there exists a polynomial time 2-approximator for \textsc{Satisfiability} (see \cite[Program~3.1]{book}).
\end{proof}

\begin{lemma}\label{lem:magic}
  Let $r, r_i, k, \epsilon \in \mathbb{R}^+$ with $r > 1$, $r_i > 1$, $k > 1$, and $\epsilon > 0$.
  If
  \begin{equation*}
    r_i \leq \frac{1}{1 - 2k(\frac{r - 1}{r} - 1)} \qquad \text{and} \qquad r < 1 + \frac{\epsilon}{2k(1 + \epsilon)},
  \end{equation*}
  then $r_i < 1 + \epsilon$.
\end{lemma}
\begin{proof}
  \begin{todo}
    Figure out what this magic is; the book suggests it is ``simple algebraic manipulations'', but I can't figure it out.

    One problem may be that in the discussion preceding the following theorem in the book, they describe a truth assignment satisfying at least a $1 - \epsilon$ factor of the maximum number of satisfiable clauses.
    However, in their proof, they prove that $r_i \leq 1 + \epsilon$, which implies that a truth assignment satisfies at least $\frac{1}{1 + \epsilon}$ factor of the maximum number of satisfiable clauses.
    Note that we can easily change the upper bound for $r$ in the hypothesis of this theorem, since it depends on $\alpha$, which we get to choose however we like, so that may help.
  \end{todo}
\end{proof}

\begin{theorem}[{\cite[Theorem~8.6]{book}}]\label{thm:maxcomplete}
  \textsc{Maximum 3-Satisfiability} is complete for the class of maximization problems in \APX{} under $\APr$ reductions.
\end{theorem}

We describe the idea of the proof before presenting the proof itself, since it is somewhat technical.

\begin{proofidea}
  First, we recall the enhanced gap-introducing reduction $(f_s, g_s, \epsilon)$ from \autoref{lem:gap3}, which shows that there exist a positive constant $\epsilon$ and functions $f_s$ and $g_s$ such that for any CNF formula $\phi$, if $\tau$ is a truth assignment satisfying at least $\frac{1}{1 + \epsilon}$ of the maximum number of satisfiable clauses of $f_s(\phi)$ then $g_s(\phi, \tau)$ is a satisfying assignment for $\phi$ if and only if $\phi$ is satisfiable.
  Here $f_s$ is a mapping from CNF formulae to 3-CNF formulae.

  The reduction we present is a generic reduction from an arbitrary maximization problem in \APX{} to \textsc{Maximum 3-Satisfiability}, again using a Cook-Levin transformation so that the number of satisfiable clauses in the Boolean formula corresponds in some way to the measure of a solution in the original maximization problem.
  In order to search for a ``good enough'' approximate solution for \textsc{Maximum 3-Satisfiability}, we will partition the interval in which we are certain the optimal solution lives (but which is too large) into a constant number of subintervals.
  For each subinterval we will use the Cook-Levin transformation to construct a Boolean formula which is satisfiable if and only if there is an approximate solution in that subinterval, and which exposes the approximate solution (by having Boolean variables $y_1, y_2, \dotsc, y_k$ whose truth values represent the values of the bits in the binary representation of $y$).
  Furthermore, we apply the function $f_s$ to each of those formulae in order to provide the gap guarantee discussed in the previous paragraph; this allows us to use \ldots.
  Finally, we use $g_s$ on one of the formulae corresponding to a subinterval to produce a ``good enough'' approximate solution for the original maximization problem.
\end{proofidea}
\begin{proof}[Proof of \autoref{thm:maxcomplete}]
  For the sake of brevity, we let the single letter $Q$ represent $\textsc{Maximum 3-Satisfiability}$.

  Let $P$ be a maximization problem in \APX, so we have $P = (I, S, m, \max)$.
  Let $A_P$ be the polynomial time computable $r_P$-approximator for $P$, where $r_P$ is some constant.
  We need to define an AP reduction $(f, g, \alpha)$ from $P$ to $Q$.
  We will delay the definition of the constant $\alpha$ until later in the proof, and until then we assume it has been fixed.
  We now wish to define functions $f$ and $g$ for all $r > 1$.
  Let $r_n = 1 + \alpha \cdot (r - 1)$; this is the desired upper bound of the performance ratio $R_P(x, g(x, \tau, r))$, as required by the AP condition when $R_Q(f(x, r), \tau) \leq r$.

  In the case that $r_p \leq r_n$, defining $g(x, y, r) = A_P(x)$ for all $x$ and $y$ is sufficient, and no definition of $f$ is necessary, since $R(x, g(x, y, r)) = R(x, A_P(x)) \leq r_P \leq r_n$.
  Suppose from here on that $r_P > r_n$.

  For $f$ we intend to map an instance $x$ of $P$ to a 3-CNF Boolean formula, and for $g$ we intend to map a truth assignment which satisfies a constant factor of the satisfiable clauses of $f(x, r)$ to a solution $y$ for $P$.
  In order to recover a solution $y$ for $P$ with $R_P(x, y) \leq r_n$, we need a solution whose measure is close to $m^*_P(x)$, the optimal measure of $x$.
  If we let $a(x) = m_P(x, A_P(x))$, then
  \begin{equation}\label{eq:bounds}
    a(x) \leq m^*_P(x) \leq r_P \cdot a(x),
  \end{equation}
  since $a(x)$ is the measure of a $r_P$-approximate solution to a maximization problem.
  We can't just output any solution in this range, because the approximation guarantee would be only $r_P$; we need it to be $r_n$.
  Hence we will subdivide this interval and derive a solution within a subinterval of size $r_n$.

  Let us divide the interval $[a(x), r_P \cdot a(x)]$ into $k$ subintervals,
  \begin{displaymath}
    \left[a(x), r_n \cdot a(x)\right], \left[r_n \cdot a(x), {r_n}^2 \cdot a(x)\right], \ldots, \left[{r_n}^{k - 1} \cdot a(x), r_P \cdot a(x)\right],
  \end{displaymath}
  where $k = \ceil{\log_{r_n}{r_P}}$.
  (Notice that the sizes of these subintervals are not necessarily equal; except that of the final subinterval, the sizes are only non-decreasing.
  This won't be a problem, as we only need a multiplicative constant approximation.)
  By construction, we have ${r_n}^k = {r_n}^{\ceil{\log_{r_n}{r_P}}} \geq {r_n}^{\log_{r_n}{r_P}} = r_P$, and hence $r_P \cdot a(x) \leq {r_n}^k \cdot a(x)$.
  Combining this with \autoref{eq:bounds}, we have $a(x) \leq m^*_P(x) \leq r_P \cdot a(x) \leq {r_n}^k \cdot a(x)$, so the optimal measure of $x$ must exist in one of these intervals.
  Now we construct $f$ and $g$ in such a way that for any $x \in I$, we can recover an approximate solution $y$ that lies in the same interval in which $m_P^*(x)$ lies.
  If we were to find a solution $y$ in the same interval in which the optimal measure lies, say, interval $j$, then we would have
  \begin{equation}\label{eq:subinterval}
    {r_n}^j \cdot a(x) \leq m_P(x, y) \leq m^*_P(x) \leq {r_n}^{j + 1} \cdot a(x).
  \end{equation}
  Then, since
  \begin{equation*}
    \frac{m^*_P(x)}{a(x)} \leq {r_n}^{j + 1} \qquad \text{and} \qquad {r_n}^j \leq \frac{m_P(x, y)}{a(x)}
  \end{equation*}
  we find that
  \begin{equation}\label{eq:result}
    R_P(x, y) = \frac{m_P^*(x)}{m_P(x, y)} = \frac{\frac{m^*_P(x)}{a(x)}}{\frac{m_P(x, y)}{a(x)}} \leq \frac{{r_n}^{j + 1}}{{r_n}^j} = r_n.
  \end{equation}
  So this is our goal when constructing $f$ and $g$.

  To define $f$ we need some further definitions.
  Define $B$ as in \autoref{alg:interval}, where $p$ is the polynomial which bounds the size of solutions in $S$.
  Assume without loss of generality that immediately before $B$ returns \texttt{True}, it writes $y$.
  \begin{algorithm}
    \caption{Nondeterministic polynomial time algorithm that decides if there is an approximate solution for instance $x$ of problem $P$ in interval $i$%
      \label{alg:interval}}
    \begin{algorithmic}
      \Require{$x \in I$, $i \geq 0$}
      \Statex{}
      \Function{$B$}{$x$, $i$}:
        \State{\textbf{guess} $y$ with $|y| \leq p(|x|)$}
        \State{$a \gets m_P(x, A_P(x))$}
        \If{$y \in S_P(x)$ and ${r_n}^i \cdot a \leq m_P(x, y) < {r_n}^{i + 1} \cdot a$}
          \Return{\texttt{True}}
        \Else
          { }\Return{\texttt{False}}
        \EndIf
      \EndFunction
    \end{algorithmic}
  \end{algorithm}
  Let $f_s$ be the special gap-introducing function described in the preceding discussion.
  Let $C$ be the Cook-Levin transformation from a description of non-deterministic Turing machine to an equivalent Boolean formula.
  Now we can define $f$ by
  \begin{displaymath}
    f(x, r) = \bigwedge_{i = 0}^{k - 1}{f_s(C(B(x, i)))},
  \end{displaymath}
  for all $x \in I$.
  Let us explain this function.
  Since we assume $B$ writes a solution $y$ before it returns \texttt{True}, $C(B(x, i))$ will have some variables, $y_1, y_2, \dotsc, y_{p(|x|)}$, which, correspond to the bits of a solution $y$, should one exist in subinterval $i$.
  Applying $f_s$ to this Boolean formula produces a 3-CNF formula which has a gap behavior which will be discussed later (since it depends on $g_s$).
  Observe that $f(x, r)$ is still a 3-CNF formula.
  We assume without loss of generality also that each formula $f_s(C(B(x, i)))$ has the same number of clauses, say $M$.

  Now, to determine how to define $g$, first suppose $x \in I$ and $\tau \in S_Q(f(x, r))$.
  For each $i$ with $0 \leq i \leq k - 1$, let $\phi_i = C(B(x, i))$ and $\psi_i = f_s(\phi_i)$.
  Let $\psi = f(x, r)$, so $\tau$ is a truth assignment to $\psi$ which satisfies some of the clauses.
  Suppose $g_s$ is the function from the enhanced gap-introducing reduction which produces solutions for $P$, as described in the preceding discussion.
  For any $\tau$ that satisfies a $\frac{1}{1 + \epsilon}$ factor of the maximum number of satisfiable clauses in $\psi_i$ for all $i \leq k - 1$, if we let $\tau_i = g_s(\phi_i, \tau)$ then $\tau_i$ satisfies $\phi_i$ if and only if $\phi_i$ is satisfiable.
  We will assume for now that $\tau$ does indeed satisfy a $\frac{1}{1 + \epsilon}$ factor of the maximum number of satisfiable clauses of $\psi_i$ for all $i$, and delay the proof of this fact until later.
  Furthermore, if $\phi_i$ is satisfiable via truth assignment $\tau_i$, then we can recover the bits of the solution $y$ from the appropriate variables of $\phi_i$, as described previously.
  Let $T$ be the function which outputs $y$ given $\phi_i$ and $\tau_i$.

  Let $j$ be the maximum $i$ such that $\tau_i$ satisfies $\phi_i$ (such a $j$ exists because $m^*_P(x)$ is in some interval, and for all intervals beyond that one, $\phi_i$ must not be satisfiable).
  This implies that there is a $y$ in interval $j$ with
  \begin{equation*}
    {r_n}^j \cdot a(x) \leq m_P(x, y) \leq m^*_P(x) \leq {r_n}^{j + 1} \cdot a(x),
  \end{equation*}
  which is exactly the statement of \autoref{eq:subinterval}, and hence \autoref{eq:result} follows.
  Therefore, we define $g(x, \tau, r)$ as in \autoref{alg:g}.
  Observe that $g$ is polynomial time computable since $k$, the number of intervals to search, is a constant, and all other functions are polynomial time computable.
  \begin{algorithm}
    \caption{Deterministic polynomial time algorithm that computes a $r_n$-approximate solution for $x$%
      \label{alg:g}}
    \begin{algorithmic}
      \Require{$x \in I$, a truth assignment $\tau$, $r > 1$}
      \Statex{}
      \Function{$g$}{$x$, $\tau$, $r$}:
        \For{$i \in \{0, 1, \dotsc, k - 1\}$}
          \State{$\phi_i = C(B, (x, i))$}
          \State{$\tau_i = g_s(\phi_i, \tau)$}
        \EndFor
        \State{$j \gets \max_{i = 0}^{k - 1}{\left\{i\,\middle|\, \tau_i \text{ satisfies } \phi_i\right\}}$} \\
        \hspace{1.5em}\Return{$T(\phi_j, g_s(\phi_j, \tau))$}
      \EndFunction
    \end{algorithmic}
  \end{algorithm}

  We now complete the tasks which we delayed above.
  First, we must prove that any truth assignment $\tau$ to $\psi$ satisfies at least a factor $\frac{1}{1 + \epsilon}$ of the maximum number of satisfiable clauses in each $\psi_i$ for each $i \leq k - 1$.
  Let $\tau$ be a truth assignment to $\psi$ whose performance ratio is at most $r$.
  Then
  \begin{align*}
    & \phantom{\implies} R_Q(\psi, \tau) = \frac{m^*_Q(\psi)}{m_Q(\psi, \tau)} \leq r \\
    & \implies m_Q(\psi, \tau) \geq \frac{m^*_Q(\psi)}{r}. \\
  \end{align*}
  From this, we have
  \begin{align*}
    m^*_Q(\psi) - m_Q(\psi, \tau) & \leq m^*_Q(\psi) - \frac{m^*_Q(\psi)}{r} \\
    & = m^*_Q(\psi) \left(1 - \frac{1}{r}\right) \\
    & = m^*_Q(\psi) \left(\frac{r - 1}{r}\right)\\
    & \leq kM \left(\frac{r - 1}{r}\right),
  \end{align*}
  where the final inequality is true because there are at most $M$ satisfiable clauses in each of the $k$ formulas $\phi_0, \phi_1, \dotsc, \phi_{k - 1}$.
  Now let $r_i$ be the performance ratio of $\tau$ with respect to $\psi_i$.
  Since
  \begin{equation*}
    m_Q^*(\psi) = \sum_{i = 0}^{k - 1}{m_Q^*(\psi_i)} \qquad \text{and} \qquad m_Q(\psi, \tau) = \sum_{i = 0}^{k - 1}{m_Q(\psi_i, \tau)},
  \end{equation*}
  we have
  \begin{align*}
    m^*_Q(\psi) - m_Q(\psi, \tau) & = \sum_{j = 0}^{k - 1}{m_Q^*(\psi_j)} - \sum_{j = 0}^{k - 1}{m_Q(\psi_j, \tau)} \\
    & = \sum_{j = 0}^{k - 1}{\left(m_Q^*(\psi_j) - m_Q(\psi_j, \tau)\right)} \\
    & \geq m_Q^*(\psi_i) - m_Q(\psi_i, \tau) \\
    & = m_Q^*(\psi_i) \left(\frac{r_i - 1}{r_i}\right) \\
    & \geq \frac{M}{2} \cdot \frac{r_i - 1}{r_i},
  \end{align*}
  where the last inequality follows from \autoref{lem:half}.
  Combining the upper and lower bounds, we have
  \begin{equation*}
    \frac{M}{2} \cdot \frac{r_i - 1}{r_i} \leq kM \left(\frac{r - 1}{r}\right),
  \end{equation*}
  from which we find
  \begin{equation*}
    1 - \frac{1}{r_i} \leq 2k\left(\frac{r - 1}{r}\right),
  \end{equation*}
  and hence
  \begin{equation*}
    1 - 2k\left(\frac{r - 1}{r}\right) \leq \frac{1}{r_i}.
  \end{equation*}
  Now we can upper bound $r_i$ as follows:
  \begin{equation*}
    r_i \leq \frac{1}{1 - 2k\left(\frac{r - 1}{r}\right)}
  \end{equation*}
  If we require that
  \begin{equation}\label{eq:rupper}
    r < 1 + \frac{\epsilon}{2k(1 + \epsilon)},
  \end{equation}
  then we have $r_i \leq 1 + \epsilon$, by \autoref{lem:magic}.
  In other words, $m_Q(\psi_i, \tau) \geq \frac{1}{1 + \epsilon} \cdot m^*_Q(\psi_i)$, which is what we intended to show.

  In order to achieve the upper bound on $r$ specified by \autoref{eq:rupper}, we recall that $r = 1 + \frac{r_n - 1}{\alpha}$ and notice that we must choose $\alpha$ such that
  \begin{equation*}
    r = 1 + \frac{r_n - 1}{\alpha} < 1 + \frac{\epsilon}{2k(1 + \epsilon)},
  \end{equation*}
  or
  \begin{equation*}
    \alpha > \frac{2k(1 + \epsilon)(r_n - 1)}{\epsilon}
  \end{equation*}
  We must choose $\alpha$ to be a constant that does not depend on the value of $r$, but we may choose it so that it depends on $r_P$.
  Since $k$ is defined to equal $\ceil{\log_{r_n}{r_P}}$, we have
  \begin{align*}
    k & \leq 1 + \frac{\log{r_P}}{\log{r_n}} \\
    & \leq 1 + \frac{r_n \log{r_P}}{\log{r_n}} \\
    & \leq 1 + \frac{r_n \log{r_P}}{r_n - 1} \\
    & = \frac{r_n \log{r_P} + r_n - 1}{r_n - 1} \\
    & < \frac{r_P \log{r_P} + r_P - 1}{r_n - 1},
  \end{align*}
  where the last inequality is true because we have assumed $r_n < r_P$.
  It follows that we need only choose $\alpha$ such that
  \begin{equation*}
    \alpha \geq \frac{2(r_P \log{r_P} + r_P - 1)(1 + \epsilon)}{\epsilon}.
  \end{equation*}
  We choose $\alpha$ to equal that value, and this is a constant since $r_P$ is a constant (defined by the approximation algorithm $A_P$) and $\epsilon$ is a constant (defined in a previous theorem).

  In summary, we define our reduction $(f, g, \alpha)$ as follows for all $x \in I$, $r > 1$, and $\tau \in S_Q(f(x, r))$:
  \begin{align*}
    f(x, r) & = \bigwedge_{i = 0}^{k - 1}{f_s(C(B, (x, i)))} \\
    g(x, \tau, r) & = T(\phi_j, g_s(\phi_j, \tau)) \\
    \alpha & = 2(r_P \log{r_P} + r_P - 1)\frac{1 + \epsilon}{\epsilon}
  \end{align*}
\end{proof}

\begin{theorem}[{\cite[Theorem~8.7]{book}}]\label{thm:minmax}
  For every minimization problem $P \in \APX$, where $P = (I, S, m_P, \min)$, there exists a maximization problem $Q \in \APX$ such that $P \APr Q$.
\end{theorem}
\begin{proof}
  Proof omitted; see \cite{book} for the proof.
\end{proof}
%% \begin{proof}
%%   Since $P \in \APX$, there exists an $r$-approximator, call it $A$, for $P$.
%%   For all $x \in I$, let $a(x) = m_P(x, A(x))$.
%%   This implies $\frac{a(x)}{m^*_P(x)} \leq r$ and hence $a(x) \leq r \cdot m^*_P(x)$.
%%   Define $Q$ by $Q = (I, S, m_Q, \max)$, where $m_Q$ is defined by
%%   \begin{equation*}
%%     m_Q(x, y) =
%%     \begin{cases}
%%       (k + 1) \cdot a(x) - k \cdot m_P(x, y) & \text{if } m_P(x, y) \leq a(x) \\
%%       t(x) & \text{otherwise,}
%%     \end{cases}
%%   \end{equation*}
%%   for all $x$ and $y$, where $k = \ceil{r}$.
%%   Following from the definition, we have $a(x) \leq m^*_Q(x) \leq (k + 1) \cdot a(x)$.
%%   This implies that $A$ is a $(k + 1)$-approximator for $Q$, and hence $Q \in \APX$.

%%   Define the AP reduction $(f, g, \alpha)$ from $P$ to $Q$ as follows.
%%   Let $f$ be the identity function and $\alpha = k + 1$.
%%   Define $g$ by
%%   \begin{equation*}
%%     g(x, y) =
%%     \begin{cases}
%%       y & \text{if } m_P(x, y) \leq a(x) \\
%%       A(x) & \text{otherwise,}
%%     \end{cases}
%%   \end{equation*}
%%   for all $x \in I$ and all $y \in S(f(x))$.

%%   To show that $(f, g, \alpha)$ is an AP reduction from $P$ to $Q$, suppose $y \in S(f(x))$ with $R_Q(x, y) \leq r'$ for any $r' > 1$.
%%   Our goal is to show that $R_P(x, g(x, y)) \leq 1 + \alpha \cdot (r' - 1)$.
%%   There are two cases.
%%   \begin{enumerate}
%%   \item $m_P(x, y) > a(x)$.
%%     In this case,
%%     \begin{equation*}
%%       R_P(x, g(x, y)) = R_P(x, A(x)) = R_Q(x, y) \leq r' \leq 1 + \alpha \cdot (r' - 1),
%%     \end{equation*}
%%     where the last inequality holds because $\alpha \geq 1$.
%%   \item $m_P(x, y) \leq a(x)$.
%%     In this case,
%%   \end{enumerate}
%%   \begin{todo}
%%     Complete this proof.
%%   \end{todo}
%% \end{proof}

\begin{corollary}
  \textsc{Maximum 3-Satisfiability} is complete for \APX{} under AP reductions.
\end{corollary}
\begin{proof}
  Follows from \autoref{thm:inapx}, \autoref{thm:maxcomplete}, and \autoref{thm:minmax}.
\end{proof}

\section*{About this work}

Copyright 2012 Jef{}frey Finkelstein.

This work is licensed under the Creative Commons Attribution-ShareAlike License 3.0.
Visit \mbox{\url{https://creativecommons.org/licenses/by-sa/3.0/}} to view a copy of this license.

The \LaTeX{} markup which generated this document is available on the World Wide Web at \mbox{\url{https://github.com/jfinkels/apxcompleteness}}.
It is also licensed under the Creative Commons Attribution-ShareAlike License.

The author can be contacted via email at \email{jeffreyf@bu.edu}.

\bibliographystyle{plain}
\bibliography{references}

\end{document}
